/**************Application of Custering in R**************/
setwd('C:\\Users\\a589565\\Desktop\\R-Files\\Statistical_Rigour_using_R_training_code\\Clustering\\All_doc')
getwd()

/**************Clustring - Exercise*****************/
  # Distance Calculation
  # Euclidean Distance
x1<-rnorm(30)
x2<-rnorm(30)
# dist
Euc_dist<-dist(rbind(x1,x2),method="euclidean")
Man_dist<-dist(rbind(x1,x2),method="manhattan")
Mink_dist<-dist(rbind(x1,x2),method="minkowski")
Euc_dist
Man_dist
Mink_dist

# Calculating the Cosine Similarity
install.packages('lsa')
library(lsa)
cosine(x1,x2)

#Calculating the Jacard Distance
vec1=c(1,1,1,1,1,0,0,0,0,0,0,1)
vec2=c(1,0,0,1,1,0,0,0,0,0,0,1)
install.packages('clusteval')
library(clusteval)
cluster_similarity(vec1,vec2,similarity = "jaccard")


# setwd("C:\\Users\\a589565\\Desktop\\R-Files\\Statistical_Rigour_using_R_training_code\\Clustering\\")
# getwd()
# Reading the protein data
protein <- read.table("protein.txt", sep="\t", header=TRUE)
View(head(protein))
summary(protein)

# UNITS AND SCALING
# Rescaling the dataset
# Using all the columns except the Country
vars.to.use <- colnames(protein)[-1]
pmatrix <- scale(protein[,vars.to.use])
pcenter<-attr(pmatrix,"scaled:center")
pscale<-attr(pmatrix,"scaled:scale")

# The scale() function annotates its output
# with two attributesâscaled:center
# returns the mean values of all the
# columns, and scaled:scale returns the
# standard deviations. Youâll store these
# away so you can âunscaleâ the data later
# 
# An Alternative to scaling exercise
x<-protein[-1]
View(head(x))
x<-data.matrix(x)
range_data<-function(x){(x-min(x))/(max(x)-min(x))}
min_data<-min(x)
max_data<-max(x)
x<-range_data(x)
View(head(x))
min_data
max_data
#Unscale the Data
#Unscaling the Data
# max_data<-max(x)
# min_data<-min(x)
# max_data
# min_data
unscale_data<-function(x,max_x,min_x){x*(max_x-min_x)+min_x}
x_actual<-unscale_data(x,max_data,min_data)
View(head(x_actual))

# Hierarchical clustering with hclust()
# Hierarchical clustering
d<-dist(pmatrix,method='euclidean')
pfit<-hclust(d,method = 'ward.D2')
plot(pfit,labels=protein$Country)

# The dendrogram suggests five clusters. You can draw the rectangles
# on the dendrogram using the function rect.hclust():
rect.hclust(pfit, k=5)
# To extract the members of each cluster from the hclust object, use cutree().
groups<-cutree(pfit,k=5)
print_clusters <- function(labels, k) {             	# Note: 1 
  for(i in 1:k) {
    print(paste("cluster", i))
    print(protein[labels==i,c("Country","RedMeat","Fish","Fr.Veg")])
  }
}
print_clusters(groups, 5)

# VISUALIZING CLUSTERS:
library(ggplot2)
princ<-prcomp(pmatrix)
# Calculate the principal components of the data
nComp<-2
# The predict() function will rotate the data into
# the space described by the principal components. We only
# want the projection on the first two axes.
project<-predict(princ,newdata=pmatrix)[,1:nComp]
# Create a data frame with the
# transformed data, along with
# the cluster label and country
# label of each point
project.plus <- cbind(as.data.frame(project),
                      cluster=as.factor(groups),
                      country=protein$Country)
ggplot(project.plus, aes(x=PC1, y=PC2)) +
  geom_point(aes(shape=cluster)) +
  geom_text(aes(label=country),
            hjust=0, vjust=1)
# BOOTSTRAP EVALUATION OF CLUSTERS:
# Running clusterboot() on the protein data
install.packages('fpc')
library(fpc)
#Set the desired number of cluster
kbest.p<-5
# Run clusterboot() with hclust ('clustermethod=hclustCBI') using
# Wardâs method ('method="ward"') and kbest.p clusters
# ('k=kbest.p'). Return the results in an object called cboot.hclust
cboot.hclust<-clusterboot(pmatrix,clustermethod = hclustCBI,method='ward.D2',k=kbest.p)
# The results of the clustering are in
# cboot.hclust$result. The output of the hclust()
# function is in cboot.hclust$result$result
summary(cboot.hclust$result)
summary(cboot.hclust$result$result)
# cboot.hclust$result$partition
# returns a vector of cluster
# labels.
groups<-cboot.hclust$result$partition
# The clusters are same as those produced before
print_clusters(groups, kbest.p)
# The vector of cluster stabilities
cboot.hclust$bootmean
# The count of how many times each cluster
# was dissolved. By default clusterboot()
# runs 100 bootstrap iterations.
cboot.hclust$bootbrd
# Calculating total within sum of squares
# Function to calculate squared
# distance between two vectors
sqr_edist<-function(x,y) {
  sum((x-y)^2)
}
# Function to calculate the WSS for a single
# cluster, which is represented as a matrix
# (one row for every point).
wss.cluster <- function(clustermat) {
  c0<-apply(clustermat, 2, FUN = mean)
  sum(apply(clustermat, 1, FUN=function(row){sqr_edist(row,c0)}))
}
wss.total <- function(dmatrix, labels) {
  wsstot <- 0
  k <- length(unique(labels))
  for(i in 1:k)
    wsstot <- wsstot + wss.cluster(subset(dmatrix, labels==i))
  wsstot
}

# The Calinski-Harabasz index:
totss <- function(dmatrix) {
  grandmean <- apply(dmatrix, 2, FUN=mean)
  sum(apply(dmatrix, 1, FUN=function(row){sqr_edist(row, grandmean)}))
}

ch_criterion <- function(dmatrix, kmax, method="kmeans") {
  if(!(method %in% c("kmeans", "hclust"))) {
    stop("method must be one of c('kmeans', 'hclust')")
  }
  npts <- dim(dmatrix)[1] # number of rows.
  totss <- totss(dmatrix)
  wss <- numeric(kmax)
  crit <- numeric(kmax)
  wss[1] <- (npts-1)*sum(apply(dmatrix, 2, var))
  for(k in 2:kmax) {
    if(method=="kmeans") {
      clustering<-kmeans(dmatrix, k, nstart=10, iter.max=100)
      wss[k] <- clustering$tot.withinss
    }else { # hclust
      d <- dist(dmatrix, method="euclidean")
      pfit <- hclust(d, method="ward.D2")
      labels <- cutree(pfit, k=k)
      wss[k] <- wss.total(dmatrix, labels)
    }
  }
  bss <- totss - wss
  crit.num <- bss/(0:(kmax-1))
  crit.denom <- wss/(npts - 1:kmax)
  list(crit = crit.num/crit.denom, wss = wss, totss = totss)
}

# Evaluating clusterings with different numbers of clusters
# Load the reshape2 package
# (for the melt() function).
library(reshape2)
clustcrit <- ch_criterion(pmatrix, 10, method="hclust")
# Create a data frame
# with the number of
# clusters, the CH
# criterion, and the WSS
# criterion. Weâll scale
# both the CH and WSS
# criteria to similar
# ranges so that we can
# plot them both on the
# same graph.
# 
# Calculate
# both
# criteria
# for 1â10
# clusters.
critframe <- data.frame(k=1:10, ch=scale(clustcrit$crit),
                        wss=scale(clustcrit$wss))
critframe <- melt(critframe, id.vars=c("k"),
                  variable.name="measure",
                  value.name="score")
# Use the melt() function to
# put the data frame in a
# shape suitable for ggplot
ggplot(critframe, aes(x=k, y=score, color=measure)) +
  geom_point(aes(shape=measure)) + geom_line(aes(linetype=measure)) +
  scale_x_continuous(breaks=1:10, labels=1:10)
/********************K-Means Clustering********************/
  # Run kmeans() with five clusters
  # (kbest.p=5), 100 random starts, and
  # 100 maximum iterations per run.
pclusters<-kmeans(pmatrix,kbest.p,nstart = 100,iter.max = 100)
summary(pclusters)

# pclusters$centers is a matrix
# whose rows are the centroids
# of the clusters. Note that
# pclusters$centers is in the
# scaled coordinates, not the
# original protein coordinates.
# 
pclusters$centers
pclusters$size
# pclusters$size returns
# the number of points
# in each cluster.
# Generally (though not
#            always) a good
# clustering will be fairly
# well balanced: no
# extremely small
# clusters and no
# extremely large ones.
# 
groups<-pclusters$cluster

print_clusters(groups, kbest.p)
#Here in this example both hclust and kmeans give very similar type of cluster
#but this wont be true for majority of the cases
#
#Run kmeansruns() from 1â10 clusters, and the CH
# criterion. By default, kmeansruns() uses 100 random
# starts and 100 maximum iterations per run
# 
clustering.ch<-kmeansruns(pmatrix,krange = 1:10,criterion = "ch")
clustering.ch$bestk
clustering.asw<-kmeansruns(pmatrix,krange = 1:10,criterion = "asw")
clustering.asw$bestk
# The vector of criterion values is called crit.
clustering.ch$crit
clustcrit$crit
# Compare the CH values for kmeans() and hclust().
# Theyâre not quite the same, because the two
# algorithms didnât pick the same clusters.
critframe <- data.frame(k=1:10, ch=scale(clustering.ch$crit),
                        asw=scale(clustering.asw$crit))
critframe <- melt(critframe, id.vars=c("k"),
                  variable.name="measure",
                  value.name="score")
ggplot(critframe, aes(x=k, y=score, color=measure)) +
  geom_point(aes(shape=measure)) + geom_line(aes(linetype=measure)) +
  scale_x_continuous(breaks=1:10, labels=1:10)

summary(clustering.ch)
# Applying Bayesian Information Criteria to choose the Optimal number of clusters
install.packages('mclust')
library(mclust)
# Run the function to see how many clusters
# it finds to be optimal, set it to search for
# at least 1 model and up 20.
d_clust <- Mclust(pmatrix, G=1:20)
m.best <- dim(d_clust$z)[2]
cat("model-based optimal number of clusters:", m.best, "\n")
# 4 clusters
plot(d_clust)
# USing the GAP Statistics - Determine the optimal Number oc Clusters
installed.packages('cluster')
library(cluster)
clusGap(pmatrix, kmeans, 10, B = 100, verbose = interactive())
# gap_statistic(pmatrix)
# Running clusterboot() with k-means
kbest.p<-5
cboot<-clusterboot(pmatrix, clustermethod=kmeansCBI,
                   runs=100,iter.max=100,
                   krange=kbest.p, seed=15555)
groups <- cboot$result$partition
print_clusters(cboot$result$partition, kbest.p)
cboot$bootmean
cboot$bootbrd
# Assigning new points to clusters
# A function to assign points to a cluster
# function to assign a new data point newpt to a clustering described
# by centers, a matrix where each row is a cluster centroid. If the data
# was scaled (using scale()) before clustering, then xcenter and xscale
# are the scaled:center and scaled:scale attributes, respectively.
# 
assign_cluster<-function(newpt, centers,xcenter=0,xscale=1) {
  xpt<-(newpt-xcenter)/xscale
  dists<-apply(centers,1,FUN = function(c0) {sqr_edist(c0,xpt)})
  which.min(dists)
}
# Letâs look at an example of assigning points to clusters, using synthetic data.
# A function to generate n
# points drawn from a
# multidimensional
# Gaussian distribution
# with centroid mean and
# standard deviation sd.
# The dimension of the
# distribution is given by
# the length of the vector
# mean.
# 
rnorm.multidim <- function(n, mean, sd, colstr="x") {
  ndim <- length(mean)
  data <- NULL
  for(i in 1:ndim) {
    col <- rnorm(n, mean=mean[[i]], sd=sd[[i]])
    data<-cbind(data, col)
  }
  cnames <- paste(colstr, 1:ndim, sep='')
  colnames(data) <- cnames
  data
}

mean1 <- c(1, 1, 1)
sd1 <- c(1, 2, 1)
mean2 <- c(10, -3, 5)
sd2 <- c(2, 1, 2)
mean3 <- c(-5, -5, -5)
sd3 <- c(1.5, 2, 1)
clust1 <- rnorm.multidim(100, mean1, sd1)
clust2 <- rnorm.multidim(100, mean2, sd2)
clust3 <- rnorm.multidim(100, mean3, sd3)
toydata <- rbind(clust3, rbind(clust1, clust2))

tmatrix <- scale(toydata)
tcenter <- attr(tmatrix, "scaled:center")
tscale<-attr(tmatrix, "scaled:scale")
kbest.t <- 3
tclusters <- kmeans(tmatrix, kbest.t, nstart=100, iter.max=100)
tclusters$size

unscale <- function(scaledpt, centervec, scalevec) {
  scaledpt*scalevec + centervec
}
unscale(tclusters$centers[1,], tcenter, tscale)
mean2
unscale(tclusters$centers[2,], tcenter, tscale)
mean3
unscale(tclusters$centers[3,], tcenter, tscale)
mean1
assign_cluster(rnorm.multidim(1, mean1, sd1),
               tclusters$centers,
               tcenter, tscale)
assign_cluster(rnorm.multidim(1, mean2, sd1),
               tclusters$centers,
               tcenter, tscale)
assign_cluster(rnorm.multidim(1, mean3, sd1),
               tclusters$centers,
               tcenter, tscale)
/*******************Upgrading the Code with Additional Information****************/
# Calculation of CCC/Pseudo-F
# Overall -Rsqr is basically the ratio of Between group Variance/Within Group Variance 
# Ratio of Between Var to Within Clustere
Overall_r_sqr<-clustering.ch$betweenss/clustering.ch$totss
Overall_r_sqr

# Pseudo F
F_pseudo<- ((clustering.ch$totss-clustering.ch$betweenss)/(length(clustering.ch$withinss)-1))/((clustering.ch$betweenss)/sum(clustering.ch$size)-length(clustering.ch$withinss))
F_pseudo

# At the optimal level of clusters , the Overall R-Sqr starts tapering off and CCC and Pseudo-F reaches its max

install.packages('NbClust')
library(NbClust)
NbClust(pmatrix, min.nc=2, max.nc=10, index="ccc", method="kmeans")






  
  
  
  
  
  
  