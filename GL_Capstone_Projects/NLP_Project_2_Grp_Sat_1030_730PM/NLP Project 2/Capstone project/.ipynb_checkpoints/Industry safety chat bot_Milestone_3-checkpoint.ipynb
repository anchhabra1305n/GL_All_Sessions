{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## greatlearning-Capstone Project-Industry Safety Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contnd......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Milestone 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Design and deploy the chatbot model on a platform [web or GUI]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do the data cleansing,model building and model selection part which we analsyed in previous milestones\n",
    "through the Tkinter GUI window.This will help to select the best model during continous learning and when there\n",
    "is change in the databse.\n",
    "Based on the best model identified,let us also build the chat bot window through the Tkinter GUI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and import all necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"tensorflow_hub>=0.6.0\"\n",
    "#!pip install -U textblob\n",
    "#!pip install Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary Librarires\n",
    "# Function to check whether required Librarires are already installed\n",
    "import imp\n",
    "def CheckModule(modulename):\n",
    "  try:\n",
    "      imp.find_module(modulename)\n",
    "      from importlib_metadata import version\n",
    "      print(\"Module: \" + modulename + \" Version:\" + version(modulename))\n",
    "  except ImportError:\n",
    "      !pip install modulename\n",
    "      from importlib_metadata import version\n",
    "      print(\"Module: \" + modulename + \" Version:\" + version(modulename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: importlib_metadata Version:1.7.0\n",
      "Module: nltk Version:3.5\n",
      "Requirement already satisfied: langdetect in c:\\users\\464s0395\\anaconda3\\lib\\site-packages (1.0.8)\n",
      "Requirement already satisfied: six in c:\\users\\464s0395\\anaconda3\\lib\\site-packages (from langdetect) (1.15.0)\n",
      "Requirement already satisfied: pyspellchecker in c:\\users\\464s0395\\anaconda3\\lib\\site-packages (0.5.5)\n",
      "Requirement already satisfied: bert-for-tf2 in c:\\users\\464s0395\\anaconda3\\lib\\site-packages (0.14.5)\n",
      "Requirement already satisfied: py-params>=0.9.6 in c:\\users\\464s0395\\anaconda3\\lib\\site-packages (from bert-for-tf2) (0.9.7)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in c:\\users\\464s0395\\anaconda3\\lib\\site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\464s0395\\anaconda3\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\464s0395\\anaconda3\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.47.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\464s0395\\anaconda3\\lib\\site-packages (0.1.91)\n"
     ]
    }
   ],
   "source": [
    "# Install/check required libraries\n",
    "CheckModule(\"importlib_metadata\")\n",
    "CheckModule(\"nltk\")\n",
    "!pip install langdetect\n",
    "!pip install pyspellchecker\n",
    "\n",
    "#install bert for tokenization\n",
    "\n",
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\464s0395\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import tensorflow as tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "from langdetect import detect \n",
    "from textblob import TextBlob \n",
    "from spellchecker import SpellChecker \n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from keras.layers import Activation\n",
    "# Swish activation\n",
    "from keras.backend import sigmoid\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import BatchNormalization,Flatten,Dense,Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D,GRU,SpatialDropout1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "import bert\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)# keras seed fixing\n",
    "tensorflow.random.set_seed(42)# tensorflow seed fixing requird for .h5 model save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "# Class for attention layer\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):#define our weights and biases\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):#main logic of Attention to get context vector\n",
    "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=K.softmax(et)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return K.sum(output,axis=1)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):# for computing output shape\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to import the custom activation Swish\n",
    "from keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "  return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data cleaning through function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessor(description):\n",
    "    stop_words='english'\n",
    "    description = re.sub(r'http\\S+',' ',description)                          # Removing any URLs\n",
    "    description = re.sub('[^a-zA-z0-9\\s]',' ',description)                    # Removing symbols\n",
    "    description=  re.sub(r\"\\b[A-Z]{2,}\\b\", \"\", description)                   # drop acronyms\n",
    "    description = str(description).lower()                                    # Convert all characters to lower case       \n",
    "    description = word_tokenize(description)                                  # Tokenization\n",
    "    description = [item for item in description if item not in stop_words]    # Stop words removal\n",
    "    description = [lemma.lemmatize(word=w,pos='v') for w in description]      # Lemmatization\n",
    "    description = [i for i in description if len(i)>2]                       # Removing words with less than 2 characters \n",
    "    description = [w for w in description if w.isalpha()]                     #remove non-alphabetical characters like '(', '.' or '!'\n",
    "    description = ' '.join(description)                                       # Convert list to string\n",
    "    return description \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker \n",
    "spell = SpellChecker() \n",
    "  \n",
    "def spell_check(x):\n",
    "    correct_word = []\n",
    "    mispelled_word = x.split()\n",
    "    for word in mispelled_word:\n",
    "        correct_word.append(spell.correction(word))\n",
    "    return ' '.join(correct_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "n_rare_words = 10\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "def remove_rarewords(text):\n",
    "    \"\"\"custom function to remove the rare words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models analysed during Milestone-2 are saved and loaded as .h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "model_A1=models.load_model(\"simple_NN_Alevel.h5\")\n",
    "model_PA1=models.load_model(\"simple_NN_PAlevel.h5\")\n",
    "model_A2=models.load_model(\"std_Tok_Alevel.h5\")\n",
    "model_PA2=models.load_model(\"std_Tok_PAlevel.h5\")\n",
    "model_A3=models.load_model(\"bert_Tok_Alevel.h5\")\n",
    "model_PA3=models.load_model(\"bert_Tok_PAlevel.h5\")\n",
    "\n",
    "model_A4=models.load_model(\"emb_lstm_Alevel.h5\")\n",
    "model_PA4=models.load_model(\"emb_lstm_PAlevel.h5\")\n",
    "\n",
    "model_A5=models.load_model(\"attn_Alevel.h5\",custom_objects={'attention': attention})\n",
    "model_PA5=models.load_model(\"attn_PAlevel.h5\",custom_objects={'attention': attention})\n",
    "\n",
    "model_A6=models.load_model(\"glove_Alevel.h5\",custom_objects={'attention': attention})\n",
    "model_PA6=models.load_model(\"glove_PAlevel.h5\",custom_objects={'attention': attention})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data cleaning,preprocessing and various model build functions through class module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bot:\n",
    "   \n",
    "    def dataprep():\n",
    "        global df1\n",
    "        data_path=folderPath.get()\n",
    "        df = pd.read_csv(data_path,index_col=0)\n",
    "        print(\"Text data Length:\",len(df['Description']))\n",
    "        print(\"data shape:\",df.shape)\n",
    "        df1 = df.dropna()\n",
    "        df1=df1[['Accident Level','Potential Accident Level','Description']]\n",
    "\n",
    "        df1['Accident Level'][df1['Accident Level'] == 'I'] = 1\n",
    "        df1['Accident Level'][df1['Accident Level'] == 'II'] = 2\n",
    "        df1['Accident Level'][df1['Accident Level'] == 'III'] = 3\n",
    "        df1['Accident Level'][df1['Accident Level'] == 'IV'] = 4\n",
    "        df1['Accident Level'][df1['Accident Level'] == 'V'] = 5\n",
    "\n",
    "        df1['Potential Accident Level'][df1['Potential Accident Level'] == 'I'] = 1\n",
    "        df1['Potential Accident Level'][df1['Potential Accident Level'] == 'II'] = 2\n",
    "        df1['Potential Accident Level'][df1['Potential Accident Level'] == 'III'] = 3\n",
    "        df1['Potential Accident Level'][df1['Potential Accident Level'] == 'IV'] = 4\n",
    "        df1['Potential Accident Level'][df1['Potential Accident Level'] == 'V'] = 5\n",
    "        df1['Potential Accident Level'][df1['Potential Accident Level'] == 'VI'] = 6\n",
    "        df1['Accident Level']=df1['Accident Level'].astype(int)\n",
    "        df1['Potential Accident Level']=df1['Potential Accident Level'].astype(int)\n",
    "\n",
    "        df1.drop(df1[df1['Potential Accident Level'] > 5].index, inplace = True) \n",
    "               \n",
    "        print(\"...........wait data cleaning in progress...........\") \n",
    "   #for i in df1['Description']: \n",
    "   # Language Detection \n",
    "    #lang = TextBlob(i)  \n",
    "    #print(lang.detect_language()) # use counter or print\n",
    "    #detect=lang.detect_language()\n",
    "   #if (detect != 'en'):\n",
    "      #detect.drop() \n",
    "        df1['clean_description']=  df1['Description'].apply(text_preprocessor) #call Data normalize function\n",
    "        df1['clean_description'] = df1['clean_description'].apply(lambda x: spell_check(x))# call Spell check and correction\n",
    "        df1['clean_description'] = df1['clean_description'].apply(lambda text: remove_rarewords(text))#call rare words  removal function \n",
    "        df1\n",
    "        print(\"********data cleaning completed******\") \n",
    "        global s\n",
    "        s=2\n",
    "        return df1\n",
    "      \n",
    "    def TF_IDF_simple_NN():\n",
    "        global df1\n",
    "        global X_train_S1,X_train_S2,X_test_S1,X_test_S2,y_train_1,y_test_1,y_train_2,y_test_2\n",
    "        sentences = df1['clean_description'].values # Description\n",
    "        y1 = df1.iloc[:,0:1].values # Accident Level\n",
    "        y2= df1.iloc[:,1:2].values  # Potential Accident Level\n",
    "        vec_tfidf = TfidfVectorizer(max_df=1.0, min_df=1,norm='l2',stop_words='english',max_features=15, lowercase=True, use_idf=True)\n",
    "        X = vec_tfidf.fit_transform(sentences)\n",
    "        sm_1 = SMOTE(sampling_strategy='auto', k_neighbors=1, random_state=1)\n",
    "        X_res_1, y_res_1 = sm_1.fit_resample(X, y1)# clean description vector and Accident level label pair\n",
    "        sm_2 = SMOTE(sampling_strategy='auto', random_state=1)\n",
    "        X_res_2, y_res_2 = sm_2.fit_resample(X, y2)# clean description vector and Potential Accident level label pair\n",
    "        # Label encoding of Accident level and Potential accident levels\n",
    "        y_1=to_categorical(y_res_1)#Encoding for accident level\n",
    "        y_2=to_categorical(y_res_2)#Encoding for Potential accident level\n",
    "        # Let us do the Train and test split.Oversampled X variable will be used for Train and test split\n",
    "        X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_res_1, y_1, test_size=0.10, random_state=1000)\n",
    "        X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_res_2, y_2, test_size=0.10, random_state=1000)\n",
    "        print(\"Accident level label data set:\",X_train_1.shape, X_test_1.shape,y_train_1.shape,y_test_1.shape)\n",
    "        print(\"Potential Accident level label data set:\",X_train_2.shape, X_test_2.shape,y_train_2.shape,y_test_2.shape)\n",
    "        #Sparse reorder\n",
    "        X_train_S1=X_train_1.toarray()\n",
    "        X_test_S1=X_test_1.toarray()\n",
    "        X_train_S2=X_train_2.toarray()\n",
    "        X_test_S2=X_test_2.toarray()\n",
    "        \n",
    "        \n",
    "    def standard_Token_Embeddings():\n",
    "        global df1,X_train_NLP_1,y_train_NLP_1,X_test_NLP_1,y_test_NLP_1,X_train_NLP_2,y_train_NLP_2,X_test_NLP_2,y_test_NLP_2\n",
    "        y1 = df1.iloc[:,0:1].values # Accident Level\n",
    "        y2= df1.iloc[:,1:2].values  # Potential Accident Level\n",
    "        max_features = 100 # let us fix max features for tokenization\n",
    "        maxlen = 60 ## max length\n",
    "        #Standard Tokenization method\n",
    "        tokenizer = Tokenizer(num_words=max_features)\n",
    "        tokenizer.fit_on_texts(df1['clean_description'])\n",
    "        #pad sequence and standard tokenization\n",
    "        X1 = tokenizer.texts_to_sequences(df1['clean_description'])\n",
    "        X1 = pad_sequences(X1, maxlen = maxlen,padding='post')\n",
    "        # As the data set is imbalanced and less in number we do the over sampling using SMOTE on standard tokenized words\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        sm_1 = SMOTE(sampling_strategy='auto', k_neighbors=1, random_state=1)\n",
    "        X_sm_1, y_sm_1 = sm_1.fit_resample(X1,y1)# clean description vector and Accident level label pair\n",
    "        sm_2 = SMOTE(sampling_strategy='auto',k_neighbors=1,random_state=1)\n",
    "        X_sm_2, y_sm_2 = sm_2.fit_resample(X1,y2)# clean description vector and Potential Accident level label pair\n",
    "        # Label encoding of Accident level and Potential accident levels\n",
    "        y_E1=to_categorical(y_sm_1)#Encoding for accident level\n",
    "        y_E2=to_categorical(y_sm_2)#Encoding for Potential accident level\n",
    "        print(\"Number of Accident Level Labels: \", len(y_E1))\n",
    "        print(\"Sample Label\",y_E1[0])# print a sample label\n",
    "        print(\"Number of Potential Accident Level Labels: \", len(y_E2))\n",
    "        print(\"Sample Labels\",y_E2[0])# print a sample label\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "        print(\"Word Embeddings Vocabulary size:\",vocab_size)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train_NLP_1, X_test_NLP_1, y_train_NLP_1, y_test_NLP_1 = train_test_split(X_sm_1, y_E1, test_size=0.10, random_state=1)\n",
    "        X_train_NLP_2, X_test_NLP_2, y_train_NLP_2, y_test_NLP_2 = train_test_split(X_sm_2, y_E2, test_size=0.10, random_state=1)\n",
    "        print(\"Accident level label data set:\",X_train_NLP_1.shape, X_test_NLP_1.shape,y_train_NLP_1.shape,y_test_NLP_1.shape)\n",
    "        print(\"Potential Accident level label data set:\",X_train_NLP_2.shape, X_test_NLP_2.shape,y_train_NLP_2.shape,y_test_NLP_2.shape)\n",
    "        \n",
    "        \n",
    "    def bert_token():\n",
    "        def tokenize_reviews(text_reviews):\n",
    "            return tokenizer_b.convert_tokens_to_ids(tokenizer_b.tokenize(text_reviews))\n",
    "        global X_train_NLP_1_bert,y_train_NLP_1_bert,X_test_NLP_1_bert,y_test_NLP_1_bert,X_train_NLP_2_bert,y_train_NLP_2_bert,X_test_NLP_2_bert,y_test_NLP_2_bert\n",
    "        global df1  \n",
    "        maxlen = 60 ## max length\n",
    "        y1 = df1.iloc[:,0:1].values # Accident Level\n",
    "        y2= df1.iloc[:,1:2].values  # Potential Accident Level\n",
    "        #Let us try  pretrained bert tokenization to check on whether the increase in vocabulary helps\n",
    "        BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "        bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "        vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "        to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "        tokenizer_b = BertTokenizer(vocabulary_file, to_lower_case)\n",
    "        \n",
    "        X1_bert = [tokenize_reviews(review) for review in df1['clean_description']]   \n",
    "        X1_bert = pad_sequences(X1_bert, maxlen = maxlen,padding='post')\n",
    "        print(\"Number of Samples:\", len(X1_bert))\n",
    "        print(\"Sample data:\",X1_bert[0])# print a sample\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        sm_1_bert = SMOTE(sampling_strategy='auto', k_neighbors=1, random_state=1)\n",
    "        X_sm_1_bert, y_sm_1_bert = sm_1_bert.fit_resample(X1_bert,y1)# clean description vector and Accident level label pair\n",
    "        sm_2_bert = SMOTE(sampling_strategy='auto',k_neighbors=1,random_state=1)\n",
    "        X_sm_2_bert, y_sm_2_bert = sm_2_bert.fit_resample(X1_bert,y2)# clean description vector and Potential Accident level label pai\n",
    "        # Label encoding of Accident level and Potential accident levels\n",
    "        y_E1=to_categorical(y_sm_1_bert)#Encoding for accident level\n",
    "        y_E2=to_categorical(y_sm_2_bert)#Encoding for Potential accident level\n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train_NLP_1_bert, X_test_NLP_1_bert, y_train_NLP_1_bert, y_test_NLP_1_bert = train_test_split(X_sm_1_bert, y_E1, test_size=0.10, random_state=1)\n",
    "        X_train_NLP_2_bert, X_test_NLP_2_bert, y_train_NLP_2_bert, y_test_NLP_2_bert = train_test_split(X_sm_2_bert, y_E2, test_size=0.10, random_state=1)\n",
    "        print(\"Accident level label bert data set:\",X_train_NLP_1_bert.shape, X_test_NLP_1_bert.shape,y_train_NLP_1_bert.shape,y_test_NLP_1_bert.shape)\n",
    "        print(\"Potential Accident level label bert data set:\",X_train_NLP_2_bert.shape, X_test_NLP_2_bert.shape,y_train_NLP_2_bert.shape,y_test_NLP_2_bert.shape)\n",
    "        # vocabulary size for bert tokenizer\n",
    "        vocab_size_b = len(tokenizer_b.vocab)\n",
    "        print(\"Vocabulary size Bert Tokenizer:\",vocab_size_b)\n",
    "        \n",
    "    def model_sel_A1():  \n",
    "        global X_train_S1,X_train_S2,X_test_S1,X_test_S2,y_train_1,y_test_1,y_train_2,y_test_2,Acc_A1,Acc_tA1\n",
    "        #reconstructed_model_A1=tensorflow.keras.models.load_model(\"simple_NN_Alevel.h5\")\n",
    "        simple_NN_Alevel_acc=model_A1.evaluate(X_train_S1,y_train_1)\n",
    "        simple_NN_Alevel_tacc=model_A1.evaluate(X_test_S1,y_test_1)\n",
    "        Acc_A1=round((simple_NN_Alevel_acc[1]*100),2)\n",
    "        Acc_tA1=round((simple_NN_Alevel_tacc[1]*100),2)\n",
    "                        \n",
    "                 \n",
    "    def model_sel_PA1():\n",
    "        global X_train_S1,X_train_S2,X_test_S1,X_test_S2,y_train_1,y_test_1,y_train_2,y_test_2,Acc_PA1,Acc_tPA1\n",
    "        #reconstructed_model_PA1=tensorflow.keras.models.load_model(\"simple_NN_PAlevel.h5\")\n",
    "        simple_NN_PAlevel_acc=model_PA1.evaluate(X_train_S2,y_train_2)\n",
    "        simple_NN_PAlevel_tacc=model_PA1.evaluate(X_test_S2,y_test_2)\n",
    "        Acc_PA1=round((simple_NN_PAlevel_acc[1]*100),2)\n",
    "        Acc_tPA1=round((simple_NN_PAlevel_tacc[1]*100),2)\n",
    "           \n",
    "                \n",
    "    def model_sel_A2():\n",
    "        global X_train_NLP_1,y_train_NLP_1,X_test_NLP_1,y_test_NLP_1,Acc_A2,Acc_tA2\n",
    "        #reconstructed_model_A2=tensorflow.keras.models.load_model(\"std_Tok_Alevel.h5\")\n",
    "        #load_weights('std_Tok_Alevel.h5')\n",
    "        std_token_Alevel_acc=model_A2.evaluate(X_train_NLP_1,y_train_NLP_1)\n",
    "        std_token_Alevel_tacc=model_A2.evaluate(X_test_NLP_1,y_test_NLP_1)\n",
    "        Acc_A2=round((std_token_Alevel_acc[1]*100),2)\n",
    "        Acc_tA2=round((std_token_Alevel_tacc[1]*100),2)\n",
    "        \n",
    "                \n",
    "    def model_sel_PA2():\n",
    "        global X_train_NLP_2,y_train_NLP_2,X_test_NLP_2,y_test_NLP_2,Acc_PA2,Acc_tPA2\n",
    "        #reconstructed_model_PA2=tensorflow.keras.models.load_model(\"std_Tok_PAlevel.h5\")                                          \n",
    "        std_token_PAlevel_acc=model_PA2.evaluate(X_train_NLP_2,y_train_NLP_2)\n",
    "        std_token_PAlevel_tacc=model_PA2.evaluate(X_test_NLP_2,y_test_NLP_2)\n",
    "        Acc_PA2=round((std_token_PAlevel_acc[1]*100),2)\n",
    "        Acc_tPA2=round((std_token_PAlevel_tacc[1]*100),2)\n",
    "                                               \n",
    "                                                              \n",
    "                \n",
    "    def model_sel_A3():\n",
    "        global X_train_NLP_1_bert,y_train_NLP_1_bert,X_test_NLP_1_bert,y_test_NLP_1_bert,Acc_A3,Acc_tA3\n",
    "        #reconstructed_model_A3=tensorflow.keras.models.load_model(\"bert_Tok_Alevel.h5\")\n",
    "        bert_token_Alevel_acc=model_A3.evaluate(X_train_NLP_1_bert,y_train_NLP_1_bert)\n",
    "        bert_token_Alevel_tacc=model_A3.evaluate(X_test_NLP_1_bert,y_test_NLP_1_bert)\n",
    "        Acc_A3=round((bert_token_Alevel_acc[1]*100),2)\n",
    "        Acc_tA3=round((bert_token_Alevel_tacc[1]*100),2)\n",
    "        \n",
    "             \n",
    "    def model_sel_PA3():\n",
    "        global X_train_NLP_2_bert,y_train_NLP_2_bert,X_test_NLP_2_bert,y_test_NLP_2_bert,Acc_PA3,Acc_tPA3\n",
    "        #reconstructed_model_PA3=tensorflow.keras.models.load_model(\"bert_Tok_PAlevel.h5\")\n",
    "        bert_token_PAlevel_acc=model_PA3.evaluate(X_train_NLP_2_bert,y_train_NLP_2_bert)\n",
    "        bert_token_PAlevel_tacc=model_PA3.evaluate(X_test_NLP_2_bert,y_test_NLP_2_bert)\n",
    "        Acc_PA3=round((bert_token_PAlevel_acc[1]*100),2)\n",
    "        Acc_tPA3=round((bert_token_PAlevel_tacc[1]*100),2)\n",
    "        \n",
    "        \n",
    "    def model_sel_A4():\n",
    "        global X_train_NLP_1,y_train_NLP_1,X_test_NLP_1,y_test_NLP_1,Acc_A4,Acc_tA4\n",
    "        #reconstructed_model_A4=tensorflow.keras.models.load_model(\"emb_lstm_Alevel.h5\")\n",
    "        lstm_token_Alevel_acc=model_A4.evaluate(X_train_NLP_1,y_train_NLP_1)\n",
    "        lstm_token_Alevel_tacc=model_A4.evaluate(X_test_NLP_1,y_test_NLP_1)\n",
    "        Acc_A4=round((lstm_token_Alevel_acc[1]*100),2)\n",
    "        Acc_tA4=round((lstm_token_Alevel_tacc[1]*100),2)\n",
    "        \n",
    "     \n",
    "    def model_sel_PA4():\n",
    "        global X_train_NLP_2,y_train_NLP_2,X_test_NLP_2,y_test_NLP_2,Acc_PA4,Acc_tPA4\n",
    "        #reconstructed_model_PA4=tensorflow.keras.models.load_model(\"emb_lstm_PAlevel.h5\")\n",
    "        lstm_token_PAlevel_acc=model_PA4.evaluate(X_train_NLP_2,y_train_NLP_2)\n",
    "        lstm_token_PAlevel_tacc=model_PA4.evaluate(X_test_NLP_2,y_test_NLP_2)\n",
    "        Acc_PA4=round((lstm_token_PAlevel_acc[1]*100),2)\n",
    "        Acc_tPA4=round((lstm_token_PAlevel_tacc[1]*100),2)\n",
    "        \n",
    "        \n",
    "    def model_sel_A5():\n",
    "        global X_train_NLP_1,y_train_NLP_1,X_test_NLP_1,y_test_NLP_1,Acc_A5,Acc_tA5\n",
    "        #reconstructed_model_A5=tensorflow.keras.models.load_model(\"attn_Alevel.h5\",custom_objects={'attention': attention})  \n",
    "        attn_token_Alevel_acc=model_A5.evaluate(X_train_NLP_1,y_train_NLP_1)\n",
    "        attn_token_Alevel_tacc=model_A5.evaluate(X_test_NLP_1,y_test_NLP_1)\n",
    "        Acc_A5=round((attn_token_Alevel_acc[1]*100),2)\n",
    "        Acc_tA5=round((attn_token_Alevel_tacc[1]*100),2)\n",
    "        \n",
    "     \n",
    "    def model_sel_PA5():\n",
    "        global X_train_NLP_2,y_train_NLP_2,X_test_NLP_2,y_test_NLP_2,Acc_PA5,Acc_tPA5\n",
    "        #reconstructed_model_PA5=tensorflow.keras.models.load_model(\"attn_PAlevel.h5\",custom_objects={'attention': attention})\n",
    "        attn_token_PAlevel_acc=model_PA5.evaluate(X_train_NLP_2,y_train_NLP_2)\n",
    "        attn_token_PAlevel_tacc=model_PA5.evaluate(X_test_NLP_2,y_test_NLP_2)\n",
    "        Acc_PA5=round((attn_token_PAlevel_acc[1]*100),2)\n",
    "        Acc_tPA5=round((attn_token_PAlevel_tacc[1]*100),2)\n",
    "             \n",
    "    def model_sel_A6():\n",
    "        global X_train_NLP_1,y_train_NLP_1,X_test_NLP_1,y_test_NLP_1,Acc_A6,Acc_tA6\n",
    "        #reconstructed_model_A6=tensorflow.keras.models.load_model(\"glove_Alevel.h5\",custom_objects={'attention': attention})\n",
    "        glove_token_Alevel_acc=model_A6.evaluate(X_train_NLP_1,y_train_NLP_1)\n",
    "        glove_token_Alevel_tacc=model_A6.evaluate(X_test_NLP_1,y_test_NLP_1)\n",
    "        Acc_A6=round((glove_token_Alevel_acc[1]*100),2)\n",
    "        Acc_tA6=round((glove_token_Alevel_tacc[1]*100),2)\n",
    "        \n",
    "    def model_sel_PA6():\n",
    "        global X_train_NLP_2,y_train_NLP_2,X_test_NLP_2,y_test_NLP_2,Acc_PA6,Acc_tPA6\n",
    "        #reconstructed_model_PA6=tensorflow.keras.models.load_model(\"glove_PAlevel.h5\",custom_objects={'attention': attention})\n",
    "        glove_token_PAlevel_acc=model_PA6.evaluate(X_train_NLP_2,y_train_NLP_2)\n",
    "        glove_token_PAlevel_tacc=model_PA6.evaluate(X_test_NLP_2,y_test_NLP_2)\n",
    "        Acc_PA6=round((glove_token_PAlevel_acc[1]*100),2)\n",
    "        Acc_tPA6=round((glove_token_PAlevel_tacc[1]*100),2)\n",
    "                \n",
    "    def final_model():\n",
    "        global Acc_A1,Acc_tA1,Acc_A2,Acc_tA2,Acc_A3,Acc_tA3,Acc_A4,Acc_tA4,Acc_A5,Acc_tA5,Acc_A6,Acc_tA6,A,PA\n",
    "        global Acc_PA1,Acc_tPA1,Acc_PA2,Acc_tPA2,Acc_PA3,Acc_tPA3,Acc_PA4,Acc_tPA4,Acc_PA5,Acc_tPA5,Acc_PA6,Acc_tPA6 \n",
    "        and_comparison_A1=0\n",
    "        and_comparison_A2=0\n",
    "        and_comparison_A3=0\n",
    "        and_comparison_A4=0\n",
    "        and_comparison_A5=0\n",
    "        and_comparison_A6=0\n",
    "        and_comparison_PA1=0\n",
    "        and_comparison_PA2=0\n",
    "        and_comparison_PA3=0\n",
    "        and_comparison_PA4=0\n",
    "        and_comparison_PA5=0\n",
    "        and_comparison_PA6=0\n",
    "        if(Acc_tA1 >Acc_tA2 and Acc_tA1 >Acc_tA3 and Acc_tA1 >Acc_tA4 and Acc_tA1 >Acc_tA5 and Acc_tA1 >Acc_tA6):\n",
    "            and_comparison_A1=1   \n",
    "        if(Acc_tA2 >Acc_tA1 and Acc_tA2 >Acc_tA3 and Acc_tA2 >Acc_tA4 and Acc_tA2 >Acc_tA5 and Acc_tA2 >Acc_tA6):\n",
    "            and_comparison_A2=1\n",
    "        if(Acc_tA3 >Acc_tA1 and Acc_tA3 >Acc_tA2 and Acc_tA3 >Acc_tA4 and Acc_tA3 >Acc_tA5 and Acc_tA3 >Acc_tA6):\n",
    "            and_comparison_A3=1\n",
    "        if(Acc_tA4 >Acc_tA1 and Acc_tA4 >Acc_tA2 and Acc_tA4 >Acc_tA3 and Acc_tA4 >Acc_tA5 and Acc_tA4 >Acc_tA6):\n",
    "            and_comparison_A4=1\n",
    "        if(Acc_tA5 >Acc_tA1 and Acc_tA5 >Acc_tA2 and Acc_tA5 >Acc_tA3 and Acc_tA5 >Acc_tA4 and Acc_tA5 >Acc_tA6):\n",
    "            and_comparison_A5=1\n",
    "        if(Acc_tA6 >Acc_tA1 and Acc_tA6 >Acc_tA2 and Acc_tA6 >Acc_tA3 and Acc_tA6 >Acc_tA4 and Acc_tA6 >Acc_tA5):\n",
    "            and_comparison_A6=1\n",
    "        if(Acc_tPA1 >Acc_tPA2 and Acc_tPA1 >Acc_tPA3 and Acc_tPA1 >Acc_tPA4 and Acc_tPA1 >Acc_tPA5 and Acc_tPA1 >Acc_tPA6):\n",
    "            and_comparison_PA1=1\n",
    "        if(Acc_tPA2 >Acc_tPA1 and Acc_tPA2 >Acc_tPA3 and Acc_tPA2 >Acc_tPA4 and Acc_tPA2 >Acc_tPA5 and Acc_tPA2 >Acc_tPA6):\n",
    "            and_comparison_PA2=1\n",
    "        if(Acc_tPA3 >Acc_tPA1 and Acc_tPA3 >Acc_tPA2 and Acc_tPA3 >Acc_tPA4 and Acc_tPA3 >Acc_tPA5 and Acc_tPA3 >Acc_tPA6):\n",
    "            and_comparison_PA3=1\n",
    "        if(Acc_tPA4 >Acc_tPA1 and Acc_tPA4 >Acc_tPA2 and Acc_tPA4 >Acc_tPA3 and Acc_tPA4 >Acc_tPA5 and Acc_tPA4 >Acc_tPA6):\n",
    "            and_comparison_PA4=1\n",
    "        if(Acc_tPA5 >Acc_tPA1 and Acc_tPA5 >Acc_tPA2 and Acc_tPA5 >Acc_tPA3 and Acc_tPA5 >Acc_tPA4 and Acc_tPA5 >Acc_tPA6):\n",
    "            and_comparison_PA5=1\n",
    "        if(Acc_tPA6 >Acc_tPA1 and Acc_tPA6 >Acc_tPA2 and Acc_tPA6 >Acc_tPA3 and Acc_tPA6 >Acc_tPA4 and Acc_tPA6 >Acc_tPA5):\n",
    "            and_comparison_PA6=1\n",
    "\n",
    "        if (abs(Acc_A1-Acc_tA1)<10):\n",
    "            if (and_comparison_A1==1):\n",
    "                print(\"Simple NN model is best for Accident level\")             \n",
    "                A=1\n",
    "        if (abs(Acc_A2-Acc_tA2)<10):\n",
    "            if (and_comparison_A2==1):\n",
    "                print(\"Standard Tokenizer Model is best for Accident level\")             \n",
    "                A=2\n",
    "        if (abs(Acc_A3-Acc_tA3)<10):\n",
    "            if (and_comparison_A3==1):\n",
    "                print(\"bert Tokenizer Model is best for Accident level\")             \n",
    "                A=3\n",
    "        if (abs(Acc_A4-Acc_tA4)<10):\n",
    "            if (and_comparison_A4==1):\n",
    "                print(\"LSTM Model is best for Accident level\")             \n",
    "                A=4\n",
    "        if (abs(Acc_A5-Acc_tA5)<10):\n",
    "            if (and_comparison_A5==1):\n",
    "                print(\"Attn layer Model is best for Accident level\")             \n",
    "                A=5\n",
    "        if (abs(Acc_A6-Acc_tA6)<10):\n",
    "            if (and_comparison_A6==1):\n",
    "                print(\"Glove Emb Model is best for Accident level\")             \n",
    "                A=6\n",
    "         \n",
    "        \n",
    "        if (abs(Acc_PA1-Acc_tPA1)<10):\n",
    "            if (and_comparison_PA1==1):\n",
    "                print(\"Simple NN model is best for P-Accident level\")             \n",
    "                PA=1\n",
    "        if (abs(Acc_PA2-Acc_tPA2)<10):\n",
    "            if (and_comparison_PA2==1):\n",
    "                print(\"Standard Tokenizer Model is best for P-Accident level\")             \n",
    "                PA=2\n",
    "        if (abs(Acc_PA3-Acc_tPA3)<10):\n",
    "            if (and_comparison_PA3==1):\n",
    "                print(\"bert Tokenizer Model is best for P-Accident level\")             \n",
    "                PA=3\n",
    "        if (abs(Acc_PA4-Acc_tPA4)<10):\n",
    "            if (and_comparison_PA4==1):\n",
    "                print(\"LSTM Model is best for P-Accident level\")             \n",
    "                PA=4\n",
    "        if (abs(Acc_PA5-Acc_tPA5)<10):\n",
    "            if (and_comparison_PA5==1):\n",
    "                print(\"Attn layer Model is best for P-Accident level\")             \n",
    "                PA=5\n",
    "        if (abs(Acc_PA6-Acc_tPA6)<10):\n",
    "            if (and_comparison_PA6==1):\n",
    "                print(\"Glove Emb Model is best for P-Accident level\")             \n",
    "                PA=6\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tkinter GUI window to call the functions through command button and to display the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path; C:/Users/464s0395/Documents/Anaco/Capstone project/chatbot working/final/IHMStefanini_industrial_safety_and_health_database_with_accidents_description.csv\n",
      "Text data Length: 425\n",
      "data shape: (425, 10)\n",
      "...........wait data cleaning in progress...........\n",
      "********data cleaning completed******\n",
      "Accident level label data set: (1422, 15) (158, 15) (1422, 6) (158, 6)\n",
      "Potential Accident level label data set: (643, 15) (72, 15) (643, 6) (72, 6)\n",
      "Number of Accident Level Labels:  1580\n",
      "Sample Label [0. 1. 0. 0. 0. 0.]\n",
      "Number of Potential Accident Level Labels:  715\n",
      "Sample Labels [0. 0. 0. 0. 1. 0.]\n",
      "Word Embeddings Vocabulary size: 2292\n",
      "Accident level label data set: (1422, 60) (158, 60) (1422, 6) (158, 6)\n",
      "Potential Accident level label data set: (643, 60) (72, 60) (643, 6) (72, 6)\n",
      "Number of Samples: 424\n",
      "Sample data: [ 1996 12366 10838 29476  1996  2490  1996  7783 22493 10956  1996  8208\n",
      "  2156  2023  1996 15893  2490  2028  2203  1996 12913  1996  3941  4139\n",
      "  2007  2119  2192  1996  3347  1998 23306  1996  8208  2013  2023  2023\n",
      "  2617  1996  3347  7358  2013  2049  2391  2490  1998 21245  1996  4344\n",
      "  1996 15893  2090  1996 12913  3347  1998  1996  7504  1996 18414 13344]\n",
      "Accident level label bert data set: (1422, 60) (158, 60) (1422, 6) (158, 6)\n",
      "Potential Accident level label bert data set: (643, 60) (72, 60) (643, 6) (72, 6)\n",
      "Vocabulary size Bert Tokenizer: 30522\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.1277 - accuracy: 0.9655\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1449 - accuracy: 0.9557\n",
      "21/21 [==============================] - 0s 20ms/step - loss: 0.3201 - accuracy: 0.8958\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4199 - accuracy: 0.9028\n",
      "45/45 [==============================] - 96s 2s/step - loss: 3.1618 - accuracy: 0.2300\n",
      "5/5 [==============================] - 8s 2s/step - loss: 3.0760 - accuracy: 0.2025\n",
      "21/21 [==============================] - 39s 2s/step - loss: 2.9264 - accuracy: 0.2364\n",
      "3/3 [==============================] - 3s 834ms/step - loss: 3.1002 - accuracy: 0.2778\n",
      "45/45 [==============================] - 85s 2s/step - loss: 4.6401 - accuracy: 0.3136\n",
      "5/5 [==============================] - 8s 2s/step - loss: 5.3720 - accuracy: 0.2785\n",
      "21/21 [==============================] - 37s 2s/step - loss: 2.2063 - accuracy: 0.5754\n",
      "3/3 [==============================] - 2s 758ms/step - loss: 1.8583 - accuracy: 0.5139\n",
      "45/45 [==============================] - 90s 2s/step - loss: 3.2043 - accuracy: 0.2405\n",
      "5/5 [==============================] - 8s 2s/step - loss: 2.9412 - accuracy: 0.2595\n",
      "21/21 [==============================] - 45s 2s/step - loss: 2.9813 - accuracy: 0.2208\n",
      "3/3 [==============================] - 3s 906ms/step - loss: 3.2715 - accuracy: 0.1528\n",
      "45/45 [==============================] - 6s 132ms/step - loss: 5.5104 - accuracy: 0.2609\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 4.9677 - accuracy: 0.2342\n",
      "21/21 [==============================] - 3s 126ms/step - loss: 8.3575 - accuracy: 0.2348\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 8.0929 - accuracy: 0.2083\n",
      "45/45 [==============================] - 1s 31ms/step - loss: 6.7516 - accuracy: 0.2082\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.8677 - accuracy: 0.1709\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 4.1730 - accuracy: 0.2193\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 3.8223 - accuracy: 0.2778\n",
      "Simple NN model is best for Accident level\n",
      "Simple NN model is best for P-Accident level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image,ImageTk\n",
    "gui =tk.Tk()\n",
    "gui.geometry(\"1500x1500\")\n",
    "#Set window background color \n",
    "gui.config(background = \"orange\") \n",
    "gui.title(\"Industry_safety_chat_bot-Model Building\")\n",
    "text = Text(gui)  \n",
    "import sys\n",
    "\n",
    "# Intializing global variables\n",
    "\n",
    "s=0\n",
    "G=0\n",
    "\n",
    "df1=[]\n",
    "X_train_S1=[]\n",
    "X_test_S1=[]\n",
    "X_train_S2=[]\n",
    "X_test_S2=[]\n",
    "y_train_1=[]\n",
    "y_test_1=[]\n",
    "y_train_2=[]\n",
    "y_test_2=[]\n",
    "X_train_NLP_1=[]\n",
    "y_train_NLP_1=[]\n",
    "X_test_NLP_1=[]\n",
    "y_test_NLP_1=[]\n",
    "X_train_NLP_2=[]\n",
    "y_train_NLP_2=[]\n",
    "X_test_NLP_2=[]\n",
    "y_test_NLP_2=[]\n",
    "X_train_NLP_1_bert=[]\n",
    "y_train_NLP_1_bert=[]\n",
    "X_test_NLP_1_bert=[]\n",
    "y_test_NLP_1_bert=[]\n",
    "X_train_NLP_2_bert=[]\n",
    "y_train_NLP_2_bert=[]\n",
    "X_test_NLP_2_bert=[]\n",
    "y_test_NLP_2_bert=[]\n",
    "\n",
    "Acc_A1=0\n",
    "Acc_tA1=0\n",
    "Acc_PA1=0\n",
    "Acc_tPA1=0\n",
    "\n",
    "Acc_A2=0\n",
    "Acc_tA2=0\n",
    "Acc_PA2=0\n",
    "Acc_tPA2=0\n",
    "\n",
    "Acc_A3=0\n",
    "Acc_tA3=0\n",
    "Acc_PA3=0\n",
    "Acc_tPA3=0\n",
    "\n",
    "Acc_A4=0\n",
    "Acc_tA4=0\n",
    "Acc_PA4=0\n",
    "Acc_tPA4=0\n",
    "\n",
    "Acc_A5=0\n",
    "Acc_tA5=0\n",
    "Acc_PA5=0\n",
    "Acc_tPA5=0\n",
    "\n",
    "Acc_A6=0\n",
    "Acc_tA6=0\n",
    "Acc_PA6=0\n",
    "Acc_tPA6=0\n",
    "\n",
    "A=0\n",
    "PA=0\n",
    "sel=0\n",
    "\n",
    "# Logo image\n",
    "\n",
    "#from Tkinter import *\n",
    "import PIL\n",
    "import PIL.ImageTk\n",
    "#Load image as PIL, then create PhotoImage from that\n",
    "\n",
    "#root = Toplevel()\n",
    "catPIL = PIL.Image.open('gl.bmp')\n",
    "cat = PIL.ImageTk.PhotoImage(catPIL)\n",
    "\n",
    "canvas = Canvas(width=200, height=50)\n",
    "canvas.grid(row=0, column=2)\n",
    "\n",
    "#Tkinter uses PhotoImage objects\n",
    "#anchor is the the part of image that goes at (x,y)\n",
    "canvas.create_image((10,10), image=cat, anchor='nw')\n",
    "\n",
    "\n",
    "# functions for file upload,datacleaning activation based on tkinter button \n",
    "\n",
    "\n",
    "def getFolderPath():\n",
    "    folder_selected = filedialog.askopenfilename()\n",
    "    folderPath.set(folder_selected)\n",
    "    E = Label(gui)\n",
    "    E.grid(row=52,column=1)\n",
    "    E.config(text=folder_selected,anchor=W, justify=LEFT,width=55)\n",
    "\n",
    "def doCapture():\n",
    "    folder =folderPath.get()\n",
    "    print(\"File path;\", folder)\n",
    "    \n",
    "# status call functions    \n",
    "    \n",
    "def status():\n",
    "    global s\n",
    "    s=1\n",
    "    if(s==1):\n",
    "        btnFind3['text']='STATUS:....data cleaning in Progress...'\n",
    "        gui.after(2000, chk)\n",
    " \n",
    " \n",
    "def chk():\n",
    "    bot.dataprep()\n",
    "    btnFind3['text']='STATUS:....data cleaning completed...'\n",
    "    \n",
    "def ECapture():\n",
    "    global s\n",
    "    global df1\n",
    "    df2=pd.DataFrame(df1)\n",
    "    df2.to_csv(\"clean_description.csv\")\n",
    "    s=0\n",
    "    \n",
    "    \n",
    "#GUI layout -functions call through buttons and status display in GUI    \n",
    "folderPath = StringVar()\n",
    "folderExport = StringVar()\n",
    "subtitle1=Label(gui,text=\"1.UPLOAD DATABASE\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 16))\n",
    "subtitle1.grid(row=51,column=0)\n",
    "\n",
    "a = Label(gui ,text=\"File path:\")\n",
    "a.grid(row=52,column = 0)\n",
    "b = Label(gui ,text=\"note:Keep the file path short\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 10))\n",
    "b.grid(row=53,column = 0)\n",
    "\n",
    "btnFind = ttk.Button(gui,\n",
    "                     text=\"Browse File\",command=getFolderPath)\n",
    "\n",
    "btnFind.grid(row=52,column=2)\n",
    "\n",
    "c = ttk.Button(gui ,text=\"Import\", command=doCapture)\n",
    "c.grid(row=55,column=2)\n",
    "\n",
    "\n",
    "subtitle2=Label(gui,text=\"2.DATA CLEANSING\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 16))\n",
    "subtitle2.grid(row=56,column=0)\n",
    "\n",
    "btnFind2 = ttk.Button(gui,text=\"Clean the data\",command=status)\n",
    "btnFind2.grid(row=57,column=0)\n",
    "\n",
    "btnFind3 = Button(gui,bg='orange',fg='white')\n",
    "btnFind3['text']='STATUS:'\n",
    "btnFind3.grid(row=57,column=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "c1 = ttk.Button(gui ,text=\"Export\", command=ECapture)\n",
    "c1.grid(row=58,column=2)\n",
    "\n",
    "subtitle3=Label(gui,text=\"3.Data Preprocessing\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 16))\n",
    "subtitle3.grid(row=60,column=0)\n",
    "\n",
    "\n",
    "\n",
    "def action():\n",
    "    gui.after(2000,bot.TF_IDF_simple_NN)\n",
    "    gui.after(2000,bot.standard_Token_Embeddings)\n",
    "    gui.after(2000,bot.bert_token)\n",
    "    btnFind4['text']='STATUS:Tokenization/Train Test split in progress'\n",
    "    global G\n",
    "    G=1\n",
    "    gui.after(4000,chk1)\n",
    "\n",
    "    \n",
    "def chk1(): \n",
    "    if(G==1):\n",
    "        btnFind4['text']='STATUS:Tokenization/Train Test split completed'\n",
    "    \n",
    "    \n",
    "btnFind4 = Button(gui,bg='orange',fg='white')\n",
    "btnFind4['text']='STATUS:'\n",
    "btnFind4.grid(row=62,column=3)\n",
    "\n",
    "\n",
    "\n",
    "submit_button=ttk.Button(gui,text=\"Submit\",command=action)\n",
    "submit_button.grid(row=62,column=0)\n",
    "\n",
    "# Models build and best model selection function call through GUI\n",
    "\n",
    "def selection():\n",
    "    bot.model_sel_A1()\n",
    "    btnFind5A.config(text=Acc_A1)\n",
    "    btnFind5tA.config(text=Acc_tA1)\n",
    "    bot.model_sel_PA1()\n",
    "    btnFind6PA.config(text=Acc_PA1)\n",
    "    btnFind6tPA.config(text=Acc_tPA1)\n",
    "    bot.model_sel_A2()\n",
    "    btnFind7A.config(text=Acc_A2)\n",
    "    btnFind7tA.config(text=Acc_tA2)\n",
    "    bot.model_sel_PA2()\n",
    "    btnFind8PA.config(text=Acc_PA2)\n",
    "    btnFind8tPA.config(text=Acc_tPA2)\n",
    "    bot.model_sel_A3()\n",
    "    btnFind9A.config(text=Acc_A3)\n",
    "    btnFind9tA.config(text=Acc_tA3)\n",
    "    bot.model_sel_PA3()\n",
    "    btnFind10PA.config(text=Acc_PA3)\n",
    "    btnFind10tPA.config(text=Acc_tPA3)\n",
    "    bot.model_sel_A4()\n",
    "    btnFind11A.config(text=Acc_A4)\n",
    "    btnFind11tA.config(text=Acc_tA4)\n",
    "    bot.model_sel_PA4()\n",
    "    btnFind12PA.config(text=Acc_PA4)\n",
    "    btnFind12tPA.config(text=Acc_tPA4)\n",
    "    bot.model_sel_A5()\n",
    "    btnFind13A.config(text=Acc_A5)\n",
    "    btnFind13tA.config(text=Acc_tA5)\n",
    "    bot.model_sel_PA5()\n",
    "    btnFind14PA.config(text=Acc_PA5)\n",
    "    btnFind14tPA.config(text=Acc_tPA5)\n",
    "    bot.model_sel_A6()\n",
    "    btnFind15A.config(text=Acc_A6)\n",
    "    btnFind15tA.config(text=Acc_tA6)\n",
    "    bot.model_sel_PA6()\n",
    "    btnFind16PA.config(text=Acc_PA6)\n",
    "    btnFind16tPA.config(text=Acc_tPA6)\n",
    "    btnFind_ex['text']='STATUS:Model build completed... '\n",
    "    global sel\n",
    "    sel=1\n",
    "    if(sel==1):\n",
    "        bot.final_model()\n",
    "        if (A==1):\n",
    "            btnFinal_A['text']=\"Simple NN model-A-Level\"        \n",
    "        if (A==2):\n",
    "            btnFinal_A['text']=\"Standard Tokenizer model-A-Level\"\n",
    "        if (A==3):\n",
    "            btnFinal_A['text']=\"bert Tokenizer model-A-Level\"\n",
    "        if (A==4):\n",
    "            btnFinal_A['text']=\"LSTM model-A-Level\"\n",
    "        if (A==5):\n",
    "            btnFinal_A['text']=\"Attn Layer model-A-Level\"\n",
    "        if (A==6):\n",
    "            btnFinal_A['text']=\"Glove Emb model-A-Level\"\n",
    "        if (PA==1):\n",
    "            btnFinal_PA['text']=\"Simple NN model-PA-Level\"        \n",
    "        if (PA==2):\n",
    "            btnFinal_PA['text']=\"Standard Tokenizer model-PA-Level\"\n",
    "        if (PA==3):\n",
    "            btnFinal_PA['text']=\"bert Tokenizer model-PA-Level\"\n",
    "        if (PA==4):\n",
    "            btnFinal_PA['text']=\"LSTM model-PA-Level\"\n",
    "        if (PA==5):\n",
    "            btnFinal_PA['text']=\"Attn Layer model-PA-Level\"\n",
    "        if (PA==6):\n",
    "            btnFinal_PA['text']=\"Glove Emb model-PA-Level\" \n",
    "\n",
    "subtitle4=Label(gui,text=\"4.MODEL Selection\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 16))\n",
    "subtitle4.grid(row=63,column=0)\n",
    "\n",
    "def Execute():\n",
    "    ex=1\n",
    "    if(ex==1):\n",
    "        btnFind_ex['text']='STATUS:Model build in progress... '\n",
    "        gui.after(2000,selection)\n",
    "        \n",
    " # layout for model build and slection       \n",
    "\n",
    "Execute_button=ttk.Button(gui,text=\"Execute\",command=Execute)\n",
    "Execute_button.grid(row=64,column=0)\n",
    "\n",
    "\n",
    "btnFind_ex = Button(gui,bg='orange',fg='white')\n",
    "btnFind_ex['text']='STATUS:'\n",
    "btnFind_ex.grid(row=64,column=3)\n",
    "\n",
    "\n",
    "\n",
    "subtitle5=Label(gui,text=\"Simple NN A level:\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle5.grid(row=66,column=0)\n",
    "\n",
    "btnFind5A = Button(gui,bg='orange',fg='white',text=Acc_A1)\n",
    "btnFind5A.grid(row=66,column=2)\n",
    "btnFind5tA = Button(gui,bg='orange',fg='white',text=Acc_tA1)\n",
    "btnFind5tA.grid(row=66,column=3)\n",
    "\n",
    "subtitle6=Label(gui,text=\"Simple NN PA level:\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle6.grid(row=67,column=0)\n",
    "\n",
    "btnFind6PA = Button(gui,bg='orange',fg='white',text=Acc_PA1)\n",
    "btnFind6PA.grid(row=67,column=2)\n",
    "btnFind6tPA = Button(gui,bg='orange',fg='white',text=Acc_tPA1)\n",
    "btnFind6tPA.grid(row=67,column=3)\n",
    "\n",
    "subtitle7=Label(gui,text=\"Standard Tokenizer A level:\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle7.grid(row=68,column=0)\n",
    "\n",
    "btnFind7A = Button(gui,bg='orange',fg='white',text=Acc_A2)\n",
    "btnFind7A.grid(row=68,column=2)\n",
    "btnFind7tA = Button(gui,bg='orange',fg='white',text=Acc_tA2)\n",
    "btnFind7tA.grid(row=68,column=3)\n",
    "\n",
    "subtitle8=Label(gui,text=\"Standard Tokenizer PA level:\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle8.grid(row=69,column=0)\n",
    "\n",
    "btnFind8PA = Button(gui,bg='orange',fg='white',text=Acc_PA2)\n",
    "btnFind8PA.grid(row=69,column=2)\n",
    "btnFind8tPA = Button(gui,bg='orange',fg='white',text=Acc_tPA2)\n",
    "btnFind8tPA.grid(row=69,column=3)\n",
    "\n",
    "subtitle9=Label(gui,text=\"bert Tokenizer A level:\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle9.grid(row=70,column=0)\n",
    "\n",
    "btnFind9A = Button(gui,bg='orange',fg='white',text=Acc_A3)\n",
    "btnFind9A.grid(row=70,column=2)\n",
    "btnFind9tA = Button(gui,bg='orange',fg='white',text=Acc_tA3)\n",
    "btnFind9tA.grid(row=70,column=3)\n",
    "\n",
    "subtitle10=Label(gui,text=\"bert Tokenizer PA level:\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle10.grid(row=71,column=0)\n",
    "\n",
    "btnFind10PA = Button(gui,bg='orange',fg='white',text=Acc_PA3)\n",
    "btnFind10PA.grid(row=71,column=2)\n",
    "btnFind10tPA = Button(gui,bg='orange',fg='white',text=Acc_tPA3)\n",
    "btnFind10tPA.grid(row=71,column=3)\n",
    "\n",
    "subtitle11=Label(gui,text=\"LSTM A level:\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle11.grid(row=72,column=0)\n",
    "\n",
    "btnFind11A = Button(gui,bg='orange',fg='white',text=Acc_A4)\n",
    "btnFind11A.grid(row=72,column=2)\n",
    "btnFind11tA = Button(gui,bg='orange',fg='white',text=Acc_tA4)\n",
    "btnFind11tA.grid(row=72,column=3)\n",
    "\n",
    "subtitle12=Label(gui,text=\"LSTM PA level:\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle12.grid(row=73,column=0)\n",
    "\n",
    "btnFind12PA = Button(gui,bg='orange',fg='white',text=Acc_PA4)\n",
    "btnFind12PA.grid(row=73,column=2)\n",
    "btnFind12tPA = Button(gui,bg='orange',fg='white',text=Acc_tPA4)\n",
    "btnFind12tPA.grid(row=73,column=3)\n",
    "\n",
    "subtitle13=Label(gui,text=\"Attn A level:\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle13.grid(row=74,column=0)\n",
    "\n",
    "btnFind13A = Button(gui,bg='orange',fg='white',text=Acc_A5)\n",
    "btnFind13A.grid(row=74,column=2)\n",
    "btnFind13tA = Button(gui,bg='orange',fg='white',text=Acc_tA5)\n",
    "btnFind13tA.grid(row=74,column=3)\n",
    "\n",
    "subtitle14=Label(gui,text=\"Attn PA level:\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle14.grid(row=75,column=0)\n",
    "\n",
    "btnFind14PA = Button(gui,bg='orange',fg='white',text=Acc_PA5)\n",
    "btnFind14PA.grid(row=75,column=2)\n",
    "btnFind14tPA = Button(gui,bg='orange',fg='white',text=Acc_tPA5)\n",
    "btnFind14tPA.grid(row=75,column=3)\n",
    "\n",
    "subtitle15=Label(gui,text=\"Glove A level:\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle15.grid(row=76,column=0)\n",
    "\n",
    "btnFind15A = Button(gui,bg='orange',fg='white',text=Acc_A6)\n",
    "btnFind15A.grid(row=76,column=2)\n",
    "btnFind15tA = Button(gui,bg='orange',fg='white',text=Acc_tA6)\n",
    "btnFind15tA.grid(row=76,column=3)\n",
    "\n",
    "subtitle16=Label(gui,text=\"Glove PA level:\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle16.grid(row=77,column=0)\n",
    "\n",
    "btnFind16PA = Button(gui,bg='orange',fg='white',text=Acc_PA6)\n",
    "btnFind16PA.grid(row=77,column=2)\n",
    "btnFind16tPA = Button(gui,bg='orange',fg='white',text=Acc_tPA6)\n",
    "btnFind16tPA.grid(row=77,column=3)\n",
    "\n",
    "\n",
    "subtitle17=Label(gui,text=\"MODEL NAME\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle17.grid(row=65,column=0)\n",
    "\n",
    "subtitle18=Label(gui,text=\"TRAIN ACCURACY %\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle18.grid(row=65,column=2)\n",
    "\n",
    "subtitle19=Label(gui,text=\"TEST ACCURACY %\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 12),justify=LEFT)\n",
    "subtitle19.grid(row=65,column=3)\n",
    "    \n",
    "        \n",
    "subtitle20=Label(gui,text=\"5.BEST MODEL\",fg=\"dark green\",bg=\"orange\",font=(\"Helvetica\", 16),justify=LEFT)\n",
    "subtitle20.grid(row=70,column=4)\n",
    "\n",
    "btnFinal_A = Button(gui,bg='orange',fg='white')\n",
    "btnFinal_A['text']='.......'                \n",
    "btnFinal_A.grid(row=71,column=4)\n",
    "\n",
    "btnFinal_PA = Button(gui,bg='orange',fg='white')\n",
    "btnFinal_PA['text']='.......'                \n",
    "btnFinal_PA.grid(row=72,column=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tkinter.messagebox\n",
    "   \n",
    "def complete():\n",
    "    tkinter.messagebox.showinfo(\"message\", \"best model considered...bye..\")\n",
    "\n",
    "Deploy_button=tk.Button(gui,text=\"Deploy\",command=complete)\n",
    "Deploy_button.grid(row=77,column=4)\n",
    "\n",
    "gui.mainloop()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chat bot window using Tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the tokenizers instances on the document ready for using in chat bot preidiction to be used if embedding\n",
    "#model turns out as a best model\n",
    "\n",
    "df_doc_1 = pd.read_csv('C:/Users/464s0395/Documents/Anaco/Capstone project/chatbot working/final/clean_description.csv',index_col=0)\n",
    "\n",
    "max_features = 100 # let us fix max features for tokenization\n",
    "maxlen = 60 ## max length\n",
    "#Standard Tokenization method\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(df_doc_1)\n",
    "#pad sequence and standard tokenization\n",
    "XC1 = tokenizer.texts_to_sequences(df_doc_1)\n",
    "XC1 = pad_sequences(XC1, maxlen = maxlen,padding='post')\n",
    "        \n",
    "def tokenize_reviews(text_reviews):\n",
    "            return tokenizer_b.convert_tokens_to_ids(tokenizer_b.tokenize(text_reviews))\n",
    "maxlen = 60 ## max length\n",
    "#Let us try  pretrained bert tokenization to check on whether the increase in vocabulary helps\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer_b = BertTokenizer(vocabulary_file, to_lower_case)\n",
    "XC1_bert = ([tokenize_reviews(review) for review in df_doc_1])   \n",
    "XC1_bert = pad_sequences(XC1_bert, maxlen = maxlen,padding='post')\n",
    "    \n",
    "    \n",
    "                 \n",
    "    \n",
    "        \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to link the best model prediction based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the cleaned data base as documet for Tf-IDF vectorization\n",
    "df_doc = pd.read_csv('C:/Users/464s0395/Documents/Anaco/Capstone project/chatbot working/final/clean_description.csv',index_col=0)\n",
    "\n",
    "def generate_response(user_input):\n",
    "    global a,A1_pred,PA1_pred,san\n",
    "    bot_response = ''\n",
    "    clean_sentence=text_preprocessor(user_input) #call Data normalize function\n",
    "    clean_sentence=spell_check(clean_sentence)# call Spell check and correction\n",
    "    clean_sentence =remove_rarewords(clean_sentence)#call rare words\n",
    "    clean = pd.DataFrame([clean_sentence],columns=['clean_description'])\n",
    "    a=df_doc['clean_description']\n",
    "    a.loc[-1] = clean_sentence  # adding a row\n",
    "    a.index = a.index + 1  # shifting index\n",
    "    a = a.sort_index()  # sorting by index and add the cleaned user input as index zero\n",
    "    print(a)\n",
    "    #Tf-idf vectorization\n",
    "    word_vectorizer_A= TfidfVectorizer(max_df=1.0,max_features=15,min_df=1,norm='l2',stop_words='english', lowercase=True, use_idf=True)\n",
    "    all_word_vectors_A = word_vectorizer_A.fit_transform(a)\n",
    "    all_word_vectors_A=all_word_vectors_A.toarray()\n",
    "    #fit on models based on which ever model is selected as best model for A level and PA level\n",
    "    if(A==1):\n",
    "        predict_A=model_A1.predict(all_word_vectors_A)\n",
    "        predict_A1=np.argmax(predict_A, axis=1)\n",
    "    if(PA==1):\n",
    "        predict_PA=model_PA1.predict(all_word_vectors_A)\n",
    "        predict_PA1=np.argmax(predict_PA, axis=1)\n",
    "        \n",
    "    if (A==2)or(A==4)or(A==5)or(A==6)or (PA==2)or(PA==4)or(PA==5)or(PA==6):\n",
    "        XC2 = tokenizer.texts_to_sequences(clean)\n",
    "        XC2 = pad_sequences(XC2, maxlen = maxlen,padding='post')\n",
    "        \n",
    "    if(A==3)or (PA==3):\n",
    "        XC2_bert = ([tokenize_reviews(review) for review in clean])   \n",
    "        XC2_bert = pad_sequences(XC2_bert, maxlen = maxlen,padding='post')\n",
    "        \n",
    "    if(A==2):\n",
    "        predict_A=model_A2.predict(XC2)\n",
    "        predict_A1=np.argmax(predict_A, axis=1)\n",
    "    if(PA==2):\n",
    "        predict_PA=model_PA2.predict(XC2)\n",
    "        predict_PA1=np.argmax(predict_PA, axis=1)\n",
    "    if(A==3):\n",
    "        predict_A=model_A3.predict(XC2_bert)\n",
    "        predict_A1=np.argmax(predict_A, axis=1)\n",
    "    if(PA==3):\n",
    "        predict_PA=model_PA3.predict(XC2_bert)\n",
    "        predict_PA1=np.argmax(predict_PA, axis=1)\n",
    "    if(A==4):\n",
    "        predict_A=model_A4.predict(XC2)\n",
    "        predict_A1=np.argmax(predict_A, axis=1)\n",
    "    if(PA==4):\n",
    "        predict_PA=model_PA4.predict(XC2)\n",
    "        predict_PA1=np.argmax(predict_PA, axis=1)\n",
    "    if(A==5):\n",
    "        predict_A=model_A5.predict(XC2)\n",
    "        predict_A1=np.argmax(predict_A, axis=1)\n",
    "    if(PA==5):\n",
    "        predict_PA=model_PA5.predict(XC2)\n",
    "        predict_PA1=np.argmax(predict_PA, axis=1)\n",
    "    if(A==6):\n",
    "        predict_A=model_A6.predict(XC2)\n",
    "        predict_A1=np.argmax(predict_A, axis=1)\n",
    "    if(PA==6):\n",
    "        predict_PA=model_PA6.predict(XC2)\n",
    "        predict_PA1=np.argmax(predict_PA, axis=1)\n",
    "    \n",
    "    \n",
    "        \n",
    "                        \n",
    "    A1_pred=str(predict_A1[0])\n",
    "    PA1_pred=str(predict_PA1[0])\n",
    "    print(\"Accident level:\",predict_A1[0] )\n",
    "    print(\"Potential Accident level:\",predict_PA1[0])\n",
    "\n",
    "    bot_response_A = bot_response+'Accident level : ' +A1_pred \n",
    "    bot_response_PA = bot_response+' Potential Accident level : '+PA1_pred\n",
    "    bot_response=bot_response_A + bot_response_PA\n",
    "    san=1\n",
    "     \n",
    "    return bot_response\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chat bot window Layout and fucntions call through the Tkinter GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      while remove the drill rod the jumbo for maint...\n",
      "1      while remove the drill rod the jumbo for maint...\n",
      "2      during the activation sodium sulphide pump the...\n",
      "3      the sub station locate level when the collabor...\n",
      "4      approximately the the personnel begin the task...\n",
      "                             ...                        \n",
      "435    approximately approximately when lift the kell...\n",
      "436    the collaborator move from the infrastructure ...\n",
      "437    during the environmental monitor activity the ...\n",
      "438    the employee perform the activity strip cathod...\n",
      "439    when the assistant clean the floor module the ...\n",
      "Name: clean_description, Length: 425, dtype: object\n",
      "Accident level: 1\n",
      "Potential Accident level: 4\n",
      "0                     operator cut his finger with knife\n",
      "1      while remove the drill rod the jumbo for maint...\n",
      "2      while remove the drill rod the jumbo for maint...\n",
      "3      during the activation sodium sulphide pump the...\n",
      "4      the sub station locate level when the collabor...\n",
      "                             ...                        \n",
      "436    approximately approximately when lift the kell...\n",
      "437    the collaborator move from the infrastructure ...\n",
      "438    during the environmental monitor activity the ...\n",
      "439    the employee perform the activity strip cathod...\n",
      "440    when the assistant clean the floor module the ...\n",
      "Name: clean_description, Length: 426, dtype: object\n",
      "Accident level: 1\n",
      "Potential Accident level: 5\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image,ImageTk\n",
    "chat=Tk()\n",
    "chat.geometry(\"1000x600\")\n",
    "#Set window background color \n",
    "chat.config(bg='orange') \n",
    "chat.title(\"IS CHAT BOT-ROOM\")\n",
    "san=0\n",
    "A1_pred='0'\n",
    "PA1_pred='0'\n",
    "a=''\n",
    "A2_pred='0'\n",
    "PA2_pred='0'\n",
    "\n",
    "#from Tkinter import *\n",
    "import PIL\n",
    "import PIL.ImageTk\n",
    "#Load image as PIL, then create PhotoImage from that\n",
    "\n",
    "#root = Toplevel()\n",
    "catPIL = PIL.Image.open('bot.bmp')\n",
    "cat = PIL.ImageTk.PhotoImage(catPIL)\n",
    "\n",
    "canvas = Canvas(width=300, height=100,bg='orange')\n",
    "canvas.grid(row=0, column=0)\n",
    "\n",
    "#Tkinter uses PhotoImage objects\n",
    "#anchor is the the part of image that goes at (x,y)\n",
    "canvas.create_image((20,20), image=cat, anchor='nw')\n",
    "\n",
    "\n",
    "#Rule based random greetings\n",
    "\n",
    "import random\n",
    "greeting_inputs = (\"hey\",\"hello\",\"good morning\",\"good afternoon\", \"good evening\",\"good night\", \"morning\", \"evening\",\"night\",\"hi\", \"whatsup\")\n",
    "greeting_responses = [\"hey,type in the incident report\",\"hi,shall I see the incldent report\",\"hello waiting to know the incident\",\"good day,give me the incident\",\"hi,Iam ready to read the incident\",\"hey good day,let me know the incident\", \"cheers,incident report please\", \"hello,have a good one,Whats the incident? \", \"hello,send me the incident report\", \"Welcome,give me the incident\"]\n",
    "\n",
    "def generate_greeting_response(greeting):\n",
    "    for token in greeting.split():\n",
    "        if token.lower() in greeting_inputs:\n",
    "            return random.choice(greeting_responses)\n",
    "\n",
    "#Function to converse with user and fetch the best model prediction        \n",
    "        \n",
    "def send():      \n",
    "    global san\n",
    "    dialogue=True\n",
    "    if(dialogue==True)and e.get()!='':\n",
    "        #txt.insert(END,\"\\n\"+\"BOT:hi Iam your Industry safety bot how can I help you?\")\n",
    "        e.get().lower()\n",
    "        send=\"USER : \"+e.get()\n",
    "        txt.insert(END,\"\\n\"+send,'bg_red')\n",
    "        \n",
    "        if e.get()!='bye':\n",
    "            if e.get() == 'thanks' or e.get() == 'thank you very much' or e.get() == 'thank you':\n",
    "                txt.insert(END,\"\\n\"+\"BOT : Most welcome\")\n",
    "                dialogue = False\n",
    "            else:       \n",
    "                if generate_greeting_response(e.get()) != None:\n",
    "                    txt.insert(END,\"\\n\"+\"BOT : \"+ generate_greeting_response(e.get()))\n",
    "                else:    \n",
    "                    if san==0:\n",
    "                        txt.insert(END,\"\\n\"+\"BOT: \" +generate_response(e.get()))  \n",
    "                        #a.remove(e.get())\n",
    "                        txt.insert(END,\"\\n\"+\"BOT: shall I send you the solution? \" )\n",
    "                if san==1:\n",
    "                    \n",
    "                    #if e.get()=='what is the solution?' or e.get()=='recommendation?' or e.get()=='what is your suggestion?' or e.get()== 'what is the fix?' or e.get()== 'fix?' or e.get()== 'solution?' or e.get()=='suggestion?':\n",
    "                    if e.get()=='yes' or e.get()=='yeap' or e.get=='yaa' or e.get()=='ok':    \n",
    "                        \n",
    "                        if (A1_pred=='1'):\n",
    "                            txt.insert(END,\"\\n\"+\"BOT:PROCESS TRAINING\")\n",
    "                        if (A1_pred=='2'):\n",
    "                            txt.insert(END,\"\\n\"+\"BOT:PROPER USAGE OF TOOLS\")\n",
    "                        if (A1_pred=='3'):\n",
    "                            txt.insert(END,\"\\n\"+\"BOT:SAFETY AWARENESS\")\n",
    "                        if (A1_pred=='4'):\n",
    "                            txt.insert(END,\"\\n\"+\"BOT:FOLLOW SOP\")\n",
    "                        if (A1_pred=='5'):\n",
    "                            txt.insert(END,\"\\n\"+\"BOT:PROPER WORK PERMIT & APPROVAL\")  \n",
    "                        san=0    \n",
    "                    else: \n",
    "                        if e.get()=='no' or e.get()=='not required' or e.get()=='not for this':\n",
    "                            txt.insert(END,\"\\n\"+\"BOT:ok.Send me the next incident\")\n",
    "                            san=0\n",
    "                            \n",
    "                        \n",
    "                    \n",
    "           \n",
    "                    \n",
    "        else:     \n",
    "            dialogue=False\n",
    "            txt.insert(END,\"\\n\"+\"BOT : Good bye.Take care.Wear mask.Maintain Social Distance\")   \n",
    "            txt.insert(END,\"\\n\"+\"BOT : Press Esc for quick exit\")\n",
    "            chat.bind('<Escape>', lambda e: chat.destroy())\n",
    "            txt.config(state='disabled')\n",
    "        e.delete(0,END)\n",
    "        \n",
    "\n",
    "\n",
    "# send button configuration        \n",
    "txt=Text(chat,bg='light blue',fg='dark green',font=(\"Helvetica\", 12))\n",
    "txt.grid(row=1,column=0,columnspan=2)\n",
    "\n",
    "txt.tag_config('bg_red', foreground='red')\n",
    "\n",
    "e=Entry(chat,width=100,font=(\"Helvetica\", 12))\n",
    "send=Button(chat,text=\"send\",command=send,bg='orange',fg='green',font=(\"Helvetica\", 12)).grid(row=2,column=1)\n",
    "e.grid(row=2,column=0)\n",
    "\n",
    "\n",
    "chat.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the chat bot window is created to converse with user by fetching the best model prediction automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
