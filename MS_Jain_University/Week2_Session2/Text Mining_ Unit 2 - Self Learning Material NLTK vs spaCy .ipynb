{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuWCKMvTskl9"
   },
   "source": [
    "## **Context**\n",
    "\n",
    "**Working with text data presents a number of challenges, such as the use of special characters, accented characters, or word inflections.** However, in order to extract meaningful information from text and prepare data for modeling, libraries designed for working with text data have made the process much simpler and are nowadays a crucial part of basic Natural Language Processing.\n",
    "\n",
    "\n",
    "**NLP researchers and developers have been able to simplify the process of handling and processing text data as a result of the development of these various libraries.** This has helped NLP grow rapidly and fing applications in a wide variety of domains, befitting the ubiquitous nature of text data and how mining it for information would be valuable to any line of business in the world.\n",
    "\n",
    "\n",
    "Two libraries in particular that provide great ease of use in working with text data are:\n",
    "- **NLTK**\n",
    "- **spaCy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lPFpbmFr2La"
   },
   "source": [
    "## **Objective**\n",
    "The NLTK and spaCy libraries have plenty of modules and functions that are used in text preprocessing, text mining and model building. \n",
    "\n",
    "**We are going to learn about both libraries and compare their performances on various tasks.**\n",
    "\n",
    "Let's first look at the **NLTK** Library. Before proceeding to use that library, we will have to install it into our working environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsmCO2cls-u3"
   },
   "source": [
    "## **NLTK**\n",
    "<center><img src=\"https://files.anaconda.com/production/resources/open-source/nltk-logo.svg\" width=\"450\"></img></center>\n",
    "\n",
    "Source: <a href=\"https://www.nltk.org/\">NLTK</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiHiUoiXswPh"
   },
   "source": [
    "**First, we need to install the NTLK library** using the pip command, along with the bash exclamation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3089,
     "status": "ok",
     "timestamp": 1662460962790,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "ITTeFeyVsvCC",
    "outputId": "014d5823-a89e-495e-eb00-dd66cc1a70c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\aravind\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXro2nK_tWIe"
   },
   "source": [
    "Let's check out the NLTK library version that is installed in our working environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1372,
     "status": "ok",
     "timestamp": 1671193674781,
     "user": {
      "displayName": "Anwesha Bhattacharya",
      "userId": "14255337432980175263"
     },
     "user_tz": -330
    },
    "id": "Nks_4WoltThk",
    "outputId": "72538384-15f9-4fa0-8c22-7d0a8047cad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)\n",
    "\n",
    "# Helps to create the data frames\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbiPiAMFtmXC"
   },
   "source": [
    "We have sucessfully installed the NLTK library with a version of 3.7. NLTK has large modules that we are required to download to further work on things. We will now have a look and download all the necessary modules that are required to implement NLP operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2295,
     "status": "ok",
     "timestamp": 1671193714377,
     "user": {
      "displayName": "Anwesha Bhattacharya",
      "userId": "14255337432980175263"
     },
     "user_tz": -330
    },
    "id": "1a32nF9ztfmu",
    "outputId": "e61bb886-296e-4e3b-da4d-0d4bc45730fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aravind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aravind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aravind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\aravind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\aravind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\aravind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\aravind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading the 'punkt' module that will be helpful for tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Downloading the 'stopwords' module that will be helpful for Stopwrods removal\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Downloading the 'wordnet' module that will be helpful for stemming and lemmatization\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Downloading the 'omw1.4', dependency for Tokenization\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Downloading the 'averaged_perceptron_tagger' for POS_Tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Downloading  the required modules that are used in NER tagging\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4QQFvNQjIx2"
   },
   "source": [
    "**Let's import all the necessary functions that are required to perform tasks on text data using the downloaded modules.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1671193741610,
     "user": {
      "displayName": "Anwesha Bhattacharya",
      "userId": "14255337432980175263"
     },
     "user_tz": -330
    },
    "id": "twdbYYN4jUUc"
   },
   "outputs": [],
   "source": [
    "# Helpful to remove the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Helpful in Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Helpful in Tokenization\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Used in Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# Used in NER Tagging\n",
    "from nltk.corpus import treebank_chunk\n",
    "from nltk.chunk import ne_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46gX3u6s4v98"
   },
   "source": [
    "## **spaCy**\n",
    "\n",
    "<img src=\"https://spacy.io/static/social_default-1d3b50b1eba4c2b06244425ff0c49570.jpg\"></img>\n",
    "\n",
    "Source: <a href = \"https://spacy.io/\">spaCy</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZBgpzoxo3La"
   },
   "source": [
    "**spaCy:** spaCy is an open-source software library for advanced Natural Language Processing implemented in Python and Cython. It is intended for production use, and aids in the development of programs that process and \"understand\" massive amounts of text. It can be used to create systems for information extraction and natural language interpretation, as well as to preprocess text for Deep Learning.\n",
    "\n",
    "It can do tasks like text classification and tokenization like the NLTK library can, but it also provides pre-trained language models and pipelines that users can customize. These features are not limited to the English language.\n",
    "\n",
    "Each library utilizes either time or space to improve performance. While NLTK returns results much slower than spaCy (spaCy is a memory hog!), spaCy’s performance is attributed to the fact that it was written in Cython (a high performance hybrid language of C and Python) from the ground up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kX-wxTwkiiDd"
   },
   "source": [
    "Similar to NTLK, we will need to install the spaCy library using the **pip** command along with the bash exclamation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! pip uninstall spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==3.4.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (61.2.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (2.27.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (1.0.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (8.1.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\aravind\\appdata\\roaming\\python\\python39\\site-packages (from spacy==3.4.1) (1.9.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (21.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (4.64.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy==3.4.1) (3.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy==3.4.1) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy==3.4.1) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.1) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.1) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.1) (3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.1) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.4.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy==3.4.1) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from jinja2->spacy==3.4.1) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==3.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx8ARI8kjbFH"
   },
   "source": [
    "**Let's check the spaCy library version that is installed in our working environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1671193641680,
     "user": {
      "displayName": "Anwesha Bhattacharya",
      "userId": "14255337432980175263"
     },
     "user_tz": -330
    },
    "id": "m4-u7rqLjGfo",
    "outputId": "14db4a1e-14d3-496c-e241-d99cf08b732c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noJF05qvxQkh"
   },
   "source": [
    "In the spaCy library, we have an English language module that helps to perform language model operations on English text. Let's download the English module that will help us with the operations we need to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8917,
     "status": "ok",
     "timestamp": 1662461103376,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "1Y3JzeNfi0_E",
    "outputId": "d0c2a613-0196-4d0d-b803-f0a9d01917d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.1\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.4.1) (3.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\aravind\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.27.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (61.2.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2021.10.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# To download the spacy langauge module\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2EMjmuTxuqa"
   },
   "source": [
    "**Now load the 'en_core_web_sm'. It is a small English pipeline trained on written web text (blogs, news, comments), that includes vocabulary, syntax and entities.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1160,
     "status": "ok",
     "timestamp": 1671193646416,
     "user": {
      "displayName": "Anwesha Bhattacharya",
      "userId": "14255337432980175263"
     },
     "user_tz": -330
    },
    "id": "sh16PMXTxjcx",
    "outputId": "6d2c52d8-7d62-446b-a960-b8d9fcc7afaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1e1c3a0acd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the small english corpus\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak7Snprq2V9x"
   },
   "source": [
    "**Similarly, we can download and load up other language models. Let's see how we can do this for the French language.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11833,
     "status": "ok",
     "timestamp": 1662461136736,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "W6nmJzsr2zH8",
    "outputId": "5a9f2b1c-1db4-483b-cb75-2a2a5ee53948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.4.0/fr_core_news_sm-3.4.0-py3-none-any.whl (16.3 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from fr-core-news-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (61.2.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.21.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\aravind\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.9.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.27.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (8.1.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\aravind\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.1)\n",
      "Installing collected packages: fr-core-news-sm\n",
      "  Attempting uninstall: fr-core-news-sm\n",
      "    Found existing installation: fr-core-news-sm 3.5.0\n",
      "    Uninstalling fr-core-news-sm-3.5.0:\n",
      "      Successfully uninstalled fr-core-news-sm-3.5.0\n",
      "Successfully installed fr-core-news-sm-3.4.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11788,
     "status": "ok",
     "timestamp": 1662461165784,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "91h-Uqzs27iG",
    "outputId": "a24a9d7e-7230-4c47-ecfb-6b7b36adb49f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.fr.French at 0x1e1d3f7ea30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the french language corpus\n",
    "spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFNIQLds7KVC"
   },
   "source": [
    "**Let's now import the necessary functions from the spaCy library that are required to perform tasks on text data using the downloaded modules.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7654,
     "status": "ok",
     "timestamp": 1671193606982,
     "user": {
      "displayName": "Anwesha Bhattacharya",
      "userId": "14255337432980175263"
     },
     "user_tz": -330
    },
    "id": "dlbaVmLu7KVC",
    "outputId": "0904c420-3f71-4dfa-a46e-b7612abf4451"
   },
   "outputs": [],
   "source": [
    "# In spaCy, we import a single class and then instantiate a singleton to do the processing:\n",
    "from spacy.lang.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiihD-Ocorsa"
   },
   "source": [
    "We have sucessfully installed and loaded the NLTK and spaCy libraries as well as their dependencies into our working environment. Before jumping into NLP operations, we still have to install a few other libraries that will be helpful during the text preprocessing and text mining stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLI2kCAf7KVC"
   },
   "source": [
    "**Unidecode**\n",
    "\n",
    "The unidecode module accepts unicode string values and returns a unicode string in Python 3. By using the unidecode library, we can transliterate any unicode string into the closest possible representation in ASCII text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2553,
     "status": "ok",
     "timestamp": 1662461187919,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "BNFFaGbE7KVC",
    "outputId": "ac6b0220-a46f-45a2-8d47-49add63d7b4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\aravind\\anaconda3\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yu7cqRG37KVD"
   },
   "source": [
    "**Spell Checking**\n",
    "\n",
    "This will help correct the spellings of our text using a Python library. We can install this library into our working environment using the pip command. \n",
    "\n",
    "Please run the below command for installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3818,
     "status": "ok",
     "timestamp": 1662461191733,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "bCNoQmiZ7KVD",
    "outputId": "e7bee3e5-451c-4906-d6a0-67723f688d7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\users\\aravind\\anaconda3\\lib\\site-packages (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjN5noAT7KVD"
   },
   "source": [
    "Now let's apply some text preprocessing and language model techniques on our text data using both the NLTK and spaCy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1671193570671,
     "user": {
      "displayName": "Anwesha Bhattacharya",
      "userId": "14255337432980175263"
     },
     "user_tz": -330
    },
    "id": "jj3REtaQzJrp"
   },
   "outputs": [],
   "source": [
    "text = '''When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to, said Thrun, in an interview with Recode earlier this week'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1671193580451,
     "user": {
      "displayName": "Anwesha Bhattacharya",
      "userId": "14255337432980175263"
     },
     "user_tz": -330
    },
    "id": "iE3kBdO4ztvs",
    "outputId": "faf32ae2-5841-4f15-f477-03b316e5d9c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to, said Thrun, in an interview with Recode earlier this week\n"
     ]
    }
   ],
   "source": [
    "# Let's print the data\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 1509,
     "status": "ok",
     "timestamp": 1671193653944,
     "user": {
      "displayName": "Anwesha Bhattacharya",
      "userId": "14255337432980175263"
     },
     "user_tz": -330
    },
    "id": "ihByzVar0lVX"
   },
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser and Named Entity Recognition\n",
    "en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Holds the stopwords\n",
    "sp_stopwords = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "VBXRCb6l1c9t"
   },
   "outputs": [],
   "source": [
    "# Pass the text data to nlp() parser and store the text into doc variable\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q8G90XNNWxS"
   },
   "source": [
    "spaCy provides a variety of linguistic annotations to give you insights into a text's grammatical structure. This includes word types, like the Parts of Speech, and how the words are related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lp3fYoLc7KVE"
   },
   "source": [
    "## **Stop Word Removal**\n",
    "\n",
    "Now first we shall remove the Stop words from our text data. Stop words are common words that are often omitted when text is analyzed. Both the NLTK & spaCy toolkits provide lists of stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1671193759722,
     "user": {
      "displayName": "Anwesha Bhattacharya",
      "userId": "14255337432980175263"
     },
     "user_tz": -330
    },
    "id": "88UIj9pz7KVE",
    "outputId": "0b9cbc94-5026-41bf-f790-eed88cbc972f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK\n",
    "sorted(nltk.corpus.stopwords.words('english'))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So0cPJLp7KVF"
   },
   "source": [
    "**Let's now look at the stopwords available in the spaCy library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1662461192558,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "fJZz-t3G7KVF",
    "outputId": "963c3861-ae95-4ab9-e85f-87b48894f724"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sp_stopwords)[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udPZr09H7KVF"
   },
   "source": [
    "We observe that there are lot of stopwords which do not really require us to understand the context of the text. Now let's remove these stopwords from the text data that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qpBvST67KVF"
   },
   "source": [
    "### **NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1671193838339,
     "user": {
      "displayName": "Anwesha Bhattacharya",
      "userId": "14255337432980175263"
     },
     "user_tz": -330
    },
    "id": "dHPr7Cdk6dBx"
   },
   "source": [
    "text = '''NLTK and spaCy are two of the most popular Natural Language Processing tools available in Python'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 368,
     "status": "ok",
     "timestamp": 1671193841113,
     "user": {
      "displayName": "Anwesha Bhattacharya",
      "userId": "14255337432980175263"
     },
     "user_tz": -330
    },
    "id": "MLP2zWkt7KVF"
   },
   "outputs": [],
   "source": [
    "data = text.split()\n",
    "\n",
    "# Removing the stopwords\n",
    "words = [word for word in data if not word in stopwords.words('english')]\n",
    "words = ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iU24zwpY7KVF"
   },
   "source": [
    "**Let's find out the number of words present in the text data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1671193843859,
     "user": {
      "displayName": "Anwesha Bhattacharya",
      "userId": "14255337432980175263"
     },
     "user_tz": -330
    },
    "id": "75nQq4TP7KVF",
    "outputId": "47d8d330-0271-42e0-b060-8e2d3830b884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No of Words :  56\n"
     ]
    }
   ],
   "source": [
    "print(\" No of Words : \", len(text.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRnOqV1a7KVG"
   },
   "source": [
    "**Let's find out the number of words present in the text data after removing the stopwords.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1671193846958,
     "user": {
      "displayName": "Anwesha Bhattacharya",
      "userId": "14255337432980175263"
     },
     "user_tz": -330
    },
    "id": "IDcj8-4H7KVG",
    "outputId": "c092a629-f280-47d7-c90b-8176a8ae65da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No of Words after removing stopwords : 38\n"
     ]
    }
   ],
   "source": [
    "print(\" No of Words after removing stopwords :\", len(words.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ina69hDi7KVG"
   },
   "source": [
    "So we had a total of 56 words, among them NLTK removed 18 stopwords from the text data, leaving us with 38 words remaining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y7Ga-CP7KVG"
   },
   "source": [
    "### **spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "0fgR_zxH7KVG"
   },
   "outputs": [],
   "source": [
    "data = text.split()\n",
    "\n",
    "# Removing the stopwords\n",
    "words = [word for word in data if not word in sp_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1662461192560,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "nbOMptGd7KVG",
    "outputId": "c0147f79-0001-47e2-ff50-3c79d0c51bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No of Words :  37\n"
     ]
    }
   ],
   "source": [
    "print(\" No of Words : \",len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1oH4OdZ7KVH"
   },
   "source": [
    "- spaCy ends up removing just one more stop word than NLTK, leaving us with 37 words in the end. Both libraries hence seem to remove stop words in more or less a similar manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAEJsxTsomWW"
   },
   "source": [
    "## **Tokenization**\n",
    "\n",
    "Now let's look at Tokenization, the process that helps us tokenize the data into chunks. Tokenization simply breaks down a stream of raw text into small chunks of words or sentences called tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJvecxx7hBD3"
   },
   "source": [
    "### **NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC3YQfxrpWWg"
   },
   "source": [
    "**Word Tokenization**\n",
    "\n",
    "This is the most popular method - it divides a piece of text into individual words based on a specific delimiter. Let's implement this using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1662461192561,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "LkePDSbClt_Q",
    "outputId": "d46afea8-5a42-490a-d795-158f029b20a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'Sebastian',\n",
       " 'Thrun',\n",
       " 'started',\n",
       " 'working',\n",
       " 'on',\n",
       " 'self-driving',\n",
       " 'cars',\n",
       " 'at',\n",
       " 'Google',\n",
       " 'in',\n",
       " '2007',\n",
       " ',',\n",
       " 'few',\n",
       " 'people',\n",
       " 'outside',\n",
       " 'of',\n",
       " 'the',\n",
       " 'company',\n",
       " 'took',\n",
       " 'him',\n",
       " 'seriously',\n",
       " '.',\n",
       " 'I',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'very',\n",
       " 'senior',\n",
       " 'CEOs',\n",
       " 'of',\n",
       " 'major',\n",
       " 'American',\n",
       " 'car',\n",
       " 'companies',\n",
       " 'would',\n",
       " 'shake',\n",
       " 'my',\n",
       " 'hand',\n",
       " 'and',\n",
       " 'turn',\n",
       " 'away',\n",
       " 'because',\n",
       " 'I',\n",
       " 'wasn',\n",
       " '’',\n",
       " 't',\n",
       " 'worth',\n",
       " 'talking',\n",
       " 'to',\n",
       " ',',\n",
       " 'said',\n",
       " 'Thrun',\n",
       " ',',\n",
       " 'in',\n",
       " 'an',\n",
       " 'interview',\n",
       " 'with',\n",
       " 'Recode',\n",
       " 'earlier',\n",
       " 'this',\n",
       " 'week']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Tokenization using NLTK Library\n",
    "result = word_tokenize(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1662461192561,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "J1kRKoNlmT0s",
    "outputId": "7a3a4846-e48d-4e2b-d008-2b1ed3eca1b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQanEs77mRJA"
   },
   "source": [
    "- As we see, we got 62 word tokens for this text data. \n",
    "\n",
    "**Now let's look at Sentence Tokenization and how it breaks the text down by sentence.**\n",
    "\n",
    "We shall implement this with NLTK as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1662461192562,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "E0_MQqVumj2Z",
    "outputId": "41aa4128-efd3-4f34-c448-b4837282c4ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.', 'I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to, said Thrun, in an interview with Recode earlier this week']\n",
      " \n",
      "Number of Sentence Tokens 2\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "\n",
    "result = sent_tokenize(text)\n",
    "print(result)\n",
    "\n",
    "print(\" \\nNumber of Sentence Tokens\", len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONEe7KcBrFC5"
   },
   "source": [
    "- We observe that our text data is comprosed of merely 2 sentence tokens. \n",
    "- This may hence be a more apt method for larger documents with multiple sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyrE0w9_ogQt"
   },
   "source": [
    "### **spaCy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rZ475iqokSh"
   },
   "source": [
    "Let's now apply word tokenization and sentence tokenization using the spaCy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1662461192562,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "UII1yAC1ryom",
    "outputId": "e54b746c-94e5-432e-db0b-032c29ff99e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'Sebastian',\n",
       " 'Thrun',\n",
       " 'started',\n",
       " 'working',\n",
       " 'on',\n",
       " 'self',\n",
       " '-',\n",
       " 'driving',\n",
       " 'cars',\n",
       " 'at',\n",
       " 'Google',\n",
       " 'in',\n",
       " '2007',\n",
       " ',',\n",
       " 'few',\n",
       " 'people',\n",
       " 'outside',\n",
       " 'of',\n",
       " 'the',\n",
       " 'company',\n",
       " 'took',\n",
       " 'him',\n",
       " 'seriously',\n",
       " '.',\n",
       " 'I',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'very',\n",
       " 'senior',\n",
       " 'CEOs',\n",
       " 'of',\n",
       " 'major',\n",
       " 'American',\n",
       " 'car',\n",
       " 'companies',\n",
       " 'would',\n",
       " 'shake',\n",
       " 'my',\n",
       " 'hand',\n",
       " 'and',\n",
       " 'turn',\n",
       " 'away',\n",
       " 'because',\n",
       " 'I',\n",
       " 'was',\n",
       " 'n’t',\n",
       " 'worth',\n",
       " 'talking',\n",
       " 'to',\n",
       " ',',\n",
       " 'said',\n",
       " 'Thrun',\n",
       " ',',\n",
       " 'in',\n",
       " 'an',\n",
       " 'interview',\n",
       " 'with',\n",
       " 'Recode',\n",
       " 'earlier',\n",
       " 'this',\n",
       " 'week']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = []\n",
    "for token in doc:\n",
    "    tokens.append(token.text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1662461192563,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "JsA72Jo_7KVI",
    "outputId": "ad7a7cd0-6bc9-44aa-fd57-b52762e5b8cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjBctBly7KVJ"
   },
   "source": [
    "- When compared to the NLTK library, spaCy created 63 word tokens for the given text data. \n",
    "- The difference appears to be with the hyphen character '-', NLTK did not consider that a separate token, but spaCy does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1662461192564,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "h3zlm6d7rs3k",
    "outputId": "805e40f7-d9d1-473b-9047-738b1f908145"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.',\n",
       " 'I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to, said Thrun, in an interview with Recode earlier this week']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence Tokenization using spaCy\n",
    "\n",
    "# Adding the pipeline 'sentencizer' component\n",
    "sbd = nlp.add_pipe('sentencizer')\n",
    "    \n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "content = nlp(text)\n",
    "\n",
    "# Create list of sentence tokens\n",
    "sents_list = []\n",
    "\n",
    "for sent in content.sents:\n",
    "        sents_list.append(sent.text)\n",
    "sents_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xTvop1g7KVJ"
   },
   "source": [
    "- spaCy also creates the same two sentence tokens from the text data, the way NLTK does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qtwqpra27KVJ"
   },
   "source": [
    "## **Stemming**\n",
    "\n",
    "Stemming is the process of reducing a word to its stem or root format.\n",
    "\n",
    "Let us implement Stemming using the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "zPedWDfs7KVJ"
   },
   "outputs": [],
   "source": [
    "# Creating object for the PorterStemmer() class\n",
    "ps = PorterStemmer()\n",
    "\n",
    "porter_stems = []\n",
    "\n",
    "# Word Tokenization using NLTK Library\n",
    "token_data = word_tokenize(text)\n",
    "\n",
    "for word in token_data:\n",
    "    # Taking root word from actual word\n",
    "    result = ps.stem(word)\n",
    "    # Appending the root word into porter_stems list\n",
    "    porter_stems.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1662461192565,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "UzRJLQq57KVJ",
    "outputId": "83b796bb-8da4-496d-921b-57b35e5434b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when',\n",
       " 'sebastian',\n",
       " 'thrun',\n",
       " 'start',\n",
       " 'work',\n",
       " 'on',\n",
       " 'self-driv',\n",
       " 'car',\n",
       " 'at',\n",
       " 'googl',\n",
       " 'in',\n",
       " '2007',\n",
       " ',',\n",
       " 'few',\n",
       " 'peopl',\n",
       " 'outsid',\n",
       " 'of',\n",
       " 'the',\n",
       " 'compani',\n",
       " 'took',\n",
       " 'him',\n",
       " 'serious',\n",
       " '.',\n",
       " 'i',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'veri',\n",
       " 'senior',\n",
       " 'ceo',\n",
       " 'of',\n",
       " 'major',\n",
       " 'american',\n",
       " 'car',\n",
       " 'compani',\n",
       " 'would',\n",
       " 'shake',\n",
       " 'my',\n",
       " 'hand',\n",
       " 'and',\n",
       " 'turn',\n",
       " 'away',\n",
       " 'becaus',\n",
       " 'i',\n",
       " 'wasn',\n",
       " '’',\n",
       " 't',\n",
       " 'worth',\n",
       " 'talk',\n",
       " 'to',\n",
       " ',',\n",
       " 'said',\n",
       " 'thrun',\n",
       " ',',\n",
       " 'in',\n",
       " 'an',\n",
       " 'interview',\n",
       " 'with',\n",
       " 'recod',\n",
       " 'earlier',\n",
       " 'thi',\n",
       " 'week']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Km671P4j7KVK"
   },
   "source": [
    "- As we can see, we have converted each token into its respective root word / stem. Let's create a dataframe and compare how the tokens get converted into their root words.\n",
    "- Similar to how this was done with the Porter Stemmer, you can implement this for the other available stemming techniques, such as the Snowball Stemmer and the Lancaster Stemmer.\n",
    "\n",
    "**Note:**\n",
    "Stemming techniques are not available for the spaCy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "09-CAx8e7KVK"
   },
   "outputs": [],
   "source": [
    "# Creating the data frame with actual token and its respective stemmed word\n",
    "\n",
    "df = pd.DataFrame({'tokens':token_data,'stem_words':porter_stems})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1662461192565,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "dOZ0b8yy7KVK",
    "outputId": "24781420-84c1-455c-9603-395cd753bea1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>stem_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When</td>\n",
       "      <td>when</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sebastian</td>\n",
       "      <td>sebastian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thrun</td>\n",
       "      <td>thrun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>started</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>working</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tokens stem_words\n",
       "0       When       when\n",
       "1  Sebastian  sebastian\n",
       "2      Thrun      thrun\n",
       "3    started      start\n",
       "4    working       work"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHlicAQ47KVK"
   },
   "source": [
    "## **Lemmatization**\n",
    "\n",
    "Lemmatization is similar to but subtly different from Stemming - it is the transformation that uses a dictionary to map a word’s variant back to its root format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4CK5aVH7KVK"
   },
   "source": [
    "### **NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "da8mA0Ob7KVK"
   },
   "outputs": [],
   "source": [
    "# Implemenation using nltk\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemma = []\n",
    "\n",
    "# Word tokenization using NLTK Library\n",
    "token_data = word_tokenize(text)\n",
    "\n",
    "for word in token_data:\n",
    "    # Taking lemma word from actual word\n",
    "    result = lemmatizer.lemmatize(word)\n",
    "    # Appending the lemma into result list   \n",
    "    lemma.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3T9oVUm7KVK"
   },
   "source": [
    "We have converted each token into its respective root lemma. Let's create a DataFrame and compare how the tokens get converted into their root words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "Dif2HOH47KVL"
   },
   "outputs": [],
   "source": [
    "# Creating the data frame with actual token and its respective lemma word for visibility\n",
    "new_df = pd.DataFrame({'tokens':token_data,'lemma_words':lemma})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1662461193865,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "T0slmXsi7KVL",
    "outputId": "5256b6bc-435b-43d9-e2a3-69c6ff9f4853"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Recode</td>\n",
       "      <td>Recode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>earlier</td>\n",
       "      <td>earlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>week</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tokens lemma_words\n",
       "57     with        with\n",
       "58   Recode      Recode\n",
       "59  earlier     earlier\n",
       "60     this        this\n",
       "61     week        week"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKO65tbW7KVL"
   },
   "source": [
    "- We observe that in these examples, there appears to be no change between the tokens and their lemma words, as the tokens already already appear to be in their root word format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI9d8P547KVL"
   },
   "source": [
    "### **spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "WlJP8Xqn_qOC"
   },
   "outputs": [],
   "source": [
    "# Changing the nlp pipeline to text, to get the desired output\n",
    "doc = en(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "sFQjei9r7KVL"
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "spacy_lemma = []\n",
    "\n",
    "for word in doc:\n",
    "    # Appending tokens into tokens list\n",
    "    tokens.append(word.text)\n",
    "    # Storing the lemma into spacy_lemma list\n",
    "    spacy_lemma.append(word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "ZSYFEarU7KVL"
   },
   "outputs": [],
   "source": [
    "# Creating the data frame with actual token and its respective lemma word for visibility\n",
    "sp_df = pd.DataFrame({'tokens':tokens,'lemma_words':spacy_lemma})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1662461193867,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "WrxpMPv97KVL",
    "outputId": "3f92aaa2-c62d-4bdc-edd2-8994887549b8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When</td>\n",
       "      <td>when</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sebastian</td>\n",
       "      <td>Sebastian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thrun</td>\n",
       "      <td>Thrun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>started</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>working</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tokens lemma_words\n",
       "0       When        when\n",
       "1  Sebastian   Sebastian\n",
       "2      Thrun       Thrun\n",
       "3    started       start\n",
       "4    working        work"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0sJDtE17KVL"
   },
   "source": [
    "## **Part-of-speech Tagging (POS Tagging)**\n",
    "\n",
    "Part-of-speech (POS) tagging is a popular Natural Language Processing method which refers to categorizing words in a text (corpus) in correspondence with a particular part-of-speech, depending on the definition of the word and its context.\n",
    "\n",
    "For example:\n",
    "The word **when** can be tagged as a **subordinating conjunction**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtFC2jkT7KVM"
   },
   "source": [
    "### **NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1662461193867,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "NPFJFUUZ7KVM",
    "outputId": "ab7e1cd9-e7e4-4da1-90b0-f9d55005c787"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('When', 'WRB'),\n",
       " ('Sebastian', 'JJ'),\n",
       " ('Thrun', 'NNP'),\n",
       " ('started', 'VBD'),\n",
       " ('working', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('self-driving', 'JJ'),\n",
       " ('cars', 'NNS'),\n",
       " ('at', 'IN'),\n",
       " ('Google', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('2007', 'CD'),\n",
       " (',', ','),\n",
       " ('few', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('outside', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('company', 'NN'),\n",
       " ('took', 'VBD'),\n",
       " ('him', 'PRP'),\n",
       " ('seriously', 'RB'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('tell', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('very', 'RB'),\n",
       " ('senior', 'JJ'),\n",
       " ('CEOs', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('major', 'JJ'),\n",
       " ('American', 'JJ'),\n",
       " ('car', 'NN'),\n",
       " ('companies', 'NNS'),\n",
       " ('would', 'MD'),\n",
       " ('shake', 'VB'),\n",
       " ('my', 'PRP$'),\n",
       " ('hand', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('turn', 'VB'),\n",
       " ('away', 'RB'),\n",
       " ('because', 'IN'),\n",
       " ('I', 'PRP'),\n",
       " ('wasn', 'VBP'),\n",
       " ('’', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('worth', 'NN'),\n",
       " ('talking', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " (',', ','),\n",
       " ('said', 'VBD'),\n",
       " ('Thrun', 'NNP'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('interview', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('Recode', 'NNP'),\n",
       " ('earlier', 'RBR'),\n",
       " ('this', 'DT'),\n",
       " ('week', 'NN')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = nltk.pos_tag(token_data)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7R-wOEM97KVM"
   },
   "source": [
    "### **spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1662461193868,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "HC-pJplI7KVM",
    "outputId": "339dbd8a-11f1-49e7-9730-4482ddf47246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When SCONJ\n",
      "Sebastian PROPN\n",
      "Thrun PROPN\n",
      "started VERB\n",
      "working VERB\n",
      "on ADP\n",
      "self NOUN\n",
      "- PUNCT\n",
      "driving VERB\n",
      "cars NOUN\n",
      "at ADP\n",
      "Google PROPN\n",
      "in ADP\n",
      "2007 NUM\n",
      ", PUNCT\n",
      "few ADJ\n",
      "people NOUN\n",
      "outside ADV\n",
      "of ADP\n",
      "the DET\n",
      "company NOUN\n",
      "took VERB\n",
      "him PRON\n",
      "seriously ADV\n",
      ". PUNCT\n",
      "I PRON\n",
      "can AUX\n",
      "tell VERB\n",
      "you PRON\n",
      "very ADV\n",
      "senior ADJ\n",
      "CEOs NOUN\n",
      "of ADP\n",
      "major ADJ\n",
      "American ADJ\n",
      "car NOUN\n",
      "companies NOUN\n",
      "would AUX\n",
      "shake VERB\n",
      "my PRON\n",
      "hand NOUN\n",
      "and CCONJ\n",
      "turn VERB\n",
      "away ADV\n",
      "because SCONJ\n",
      "I PRON\n",
      "was AUX\n",
      "n’t PART\n",
      "worth ADJ\n",
      "talking VERB\n",
      "to ADP\n",
      ", PUNCT\n",
      "said VERB\n",
      "Thrun PROPN\n",
      ", PUNCT\n",
      "in ADP\n",
      "an DET\n",
      "interview NOUN\n",
      "with ADP\n",
      "Recode PROPN\n",
      "earlier ADV\n",
      "this DET\n",
      "week NOUN\n"
     ]
    }
   ],
   "source": [
    "# Tagging the tokens\n",
    "for token in doc:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbkszPsb7KVM"
   },
   "source": [
    "- As we see, each token gets tagged into its respective part of speech. Let's understand a few of these tags:\n",
    "  - VERB -  verbs (all tenses and modes)\n",
    "  - NOUN — nouns (common and proper)\n",
    " - PRON — pronouns\n",
    " - ADJ — adjectives\n",
    "  -  ADV — adverbs\n",
    "  - ADP — adpositions (prepositions and postpositions)\n",
    "  - CONJ — conjunctions\n",
    "\n",
    "The other tags can also be understood by referring to [this article](https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1662461193868,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "XneNcQvk7KVM",
    "outputId": "095f514e-e5ff-4c55-fdb5-f893279c0de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbs: ['started', 'working', 'driving', 'took', 'tell', 'shake', 'turn', 'talking', 'said']\n"
     ]
    }
   ],
   "source": [
    "# You want list of Verb tokens\n",
    "print(\"Verbs:\", [token.text for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1662461193868,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "oOCGYWP8h83T",
    "outputId": "21fac07f-4156-4ba4-f8da-68d6aab35fbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: ['self', 'cars', 'people', 'company', 'CEOs', 'car', 'companies', 'hand', 'interview', 'week']\n"
     ]
    }
   ],
   "source": [
    "# You want list of Noun tokens\n",
    "print(\"Nouns:\", [token.text for token in doc if token.pos_ == \"NOUN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a53doQdgh-YL"
   },
   "source": [
    "- We had successfully tagged every word token present in the data. Now let's implement the collocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skMFusCuh-pQ"
   },
   "source": [
    "## **Collocations: Bigrams and Trigrams**\n",
    "\n",
    "What are Collocations?\n",
    "\n",
    "Collocations are groups of words occurring together many times in a document. They are calculated by the number of those groups occurring together with respect to the overall word count of the document. We also call them N-grams: continuous sequences of N words or symbols or tokens in a document.\n",
    "\n",
    "- **Unigram:** An N-gram consisting of a single item from a sequence.\n",
    "- **Bigrams:** An N-gram consisting of a combination of two words from a sequence.\n",
    "- **Trigrams:** An N-gram consisting of a combination of three words from a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gd3G5pU9N0d1"
   },
   "source": [
    "**Unigrams** are the same as the word tokens generated from the corpus. \n",
    "\n",
    "We've already tokenized the data to obtain these Unigrams, so let's now obtain Bigrams and Trigrams from the same corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vcgttdsji7ms"
   },
   "source": [
    "### **Bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1662461193869,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "LXhZ2E6th_Qn",
    "outputId": "d9ccc385-eabe-4a63-9ed3-9dc4eea18ae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('When', 'Sebastian'), ('Sebastian', 'Thrun'), ('Thrun', 'started'), ('started', 'working'), ('working', 'on'), ('on', 'self-driving'), ('self-driving', 'cars'), ('cars', 'at'), ('at', 'Google'), ('Google', 'in'), ('in', '2007'), ('2007', ','), (',', 'few'), ('few', 'people'), ('people', 'outside'), ('outside', 'of'), ('of', 'the'), ('the', 'company'), ('company', 'took'), ('took', 'him'), ('him', 'seriously'), ('seriously', '.'), ('.', 'I'), ('I', 'can'), ('can', 'tell'), ('tell', 'you'), ('you', 'very'), ('very', 'senior'), ('senior', 'CEOs'), ('CEOs', 'of'), ('of', 'major'), ('major', 'American'), ('American', 'car'), ('car', 'companies'), ('companies', 'would'), ('would', 'shake'), ('shake', 'my'), ('my', 'hand'), ('hand', 'and'), ('and', 'turn'), ('turn', 'away'), ('away', 'because'), ('because', 'I'), ('I', 'wasn'), ('wasn', '’'), ('’', 't'), ('t', 'worth'), ('worth', 'talking'), ('talking', 'to'), ('to', ','), (',', 'said'), ('said', 'Thrun'), ('Thrun', ','), (',', 'in'), ('in', 'an'), ('an', 'interview'), ('interview', 'with'), ('with', 'Recode'), ('Recode', 'earlier'), ('earlier', 'this'), ('this', 'week')]\n"
     ]
    }
   ],
   "source": [
    "Tokens = nltk.word_tokenize(text)\n",
    "output = list(nltk.bigrams(Tokens))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qXQA5tzRGWq"
   },
   "source": [
    "We observe that we have created the Bigrams i.e combinations of two words, from the available text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZhTXhYNjA--"
   },
   "source": [
    "### **Trigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1662461193870,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "FkGzdJO4jBbW",
    "outputId": "139918fa-64b8-464e-91fd-6e577f0f3897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('When', 'Sebastian', 'Thrun'), ('Sebastian', 'Thrun', 'started'), ('Thrun', 'started', 'working'), ('started', 'working', 'on'), ('working', 'on', 'self-driving'), ('on', 'self-driving', 'cars'), ('self-driving', 'cars', 'at'), ('cars', 'at', 'Google'), ('at', 'Google', 'in'), ('Google', 'in', '2007'), ('in', '2007', ','), ('2007', ',', 'few'), (',', 'few', 'people'), ('few', 'people', 'outside'), ('people', 'outside', 'of'), ('outside', 'of', 'the'), ('of', 'the', 'company'), ('the', 'company', 'took'), ('company', 'took', 'him'), ('took', 'him', 'seriously'), ('him', 'seriously', '.'), ('seriously', '.', 'I'), ('.', 'I', 'can'), ('I', 'can', 'tell'), ('can', 'tell', 'you'), ('tell', 'you', 'very'), ('you', 'very', 'senior'), ('very', 'senior', 'CEOs'), ('senior', 'CEOs', 'of'), ('CEOs', 'of', 'major'), ('of', 'major', 'American'), ('major', 'American', 'car'), ('American', 'car', 'companies'), ('car', 'companies', 'would'), ('companies', 'would', 'shake'), ('would', 'shake', 'my'), ('shake', 'my', 'hand'), ('my', 'hand', 'and'), ('hand', 'and', 'turn'), ('and', 'turn', 'away'), ('turn', 'away', 'because'), ('away', 'because', 'I'), ('because', 'I', 'wasn'), ('I', 'wasn', '’'), ('wasn', '’', 't'), ('’', 't', 'worth'), ('t', 'worth', 'talking'), ('worth', 'talking', 'to'), ('talking', 'to', ','), ('to', ',', 'said'), (',', 'said', 'Thrun'), ('said', 'Thrun', ','), ('Thrun', ',', 'in'), (',', 'in', 'an'), ('in', 'an', 'interview'), ('an', 'interview', 'with'), ('interview', 'with', 'Recode'), ('with', 'Recode', 'earlier'), ('Recode', 'earlier', 'this'), ('earlier', 'this', 'week')]\n"
     ]
    }
   ],
   "source": [
    "Tokens = nltk.word_tokenize(text)\n",
    "output = list(nltk.trigrams(Tokens))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G5jR_LSK84e"
   },
   "source": [
    "We observe that here we have created Trigrams i.e combinations of three words, from the available text.\n",
    "\n",
    "**Note:** For the spaCy library, we don't have direct functions to create Bigrams, Trigrams and N-grams. That can be implemented through simple Python functions where we use N as a control parameter to slice that number of words at a time from the whole corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75qGY9jKjLhi"
   },
   "source": [
    "## **Named Entity Recognition (NER)**\n",
    "\n",
    "Named Entity Recognition (NER) is an NLP-based technique to identify mentions of rigid designators from the text belonging to particular semantic types such as a person, location, organization etc.\n",
    "\n",
    "\n",
    "<table width=\"700\">\n",
    "<th> TYPE</th>\n",
    "<th> Description </th>\n",
    "<tr>\n",
    "<td>LOC</td>\n",
    "<td> Non-GPE locations, mountain ranges, bodies of water</td></tr>\n",
    "<tr>\n",
    "<td>MONEY</td>\n",
    "<td> Monetary values, including unit.</td></tr>\n",
    "<tr>\n",
    "<td>ORG</td>\n",
    "<td> Companies, agencies, institutions etc.</td></tr>\n",
    "<tr>\n",
    "<td>PERSON</td>\n",
    "<td> People, including fictional</td></tr>\n",
    "<tr>\n",
    "<td>NORP</td>\n",
    "<td> Nationalities or religious or political groups</td></tr>\n",
    "<tr>\n",
    "<td>FAC</td>\n",
    "<td> Buildings, airports, highways, bridge, etc.</td></tr>\n",
    "<tr>\n",
    "<td>GPE</td>\n",
    "<td> Countries, cities, states.</td></tr>\n",
    "<tr><td>LANGUAGE</td>\n",
    "<td> Any named language</td></tr>\n",
    "<tr><td>PRODUCT</td>\n",
    "<td> Objects, vehicles, foods,etc. (Not services)</td></tr>\n",
    "<tr><td>LAW</td>\n",
    "<td>Named documents made into laws</td></tr>\n",
    "<tr><td>EVENT</td>\n",
    "<td> Named hurricanes, battles, wars, sports events,etc.</td></tr>\n",
    "<tr><td>DATE</td>\n",
    "<td> Absolute or relative dates or periods</td></tr>\n",
    "<tr><td>TIME</td>\n",
    "<td> Times smaller than a day</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hC-amUXKn_xW"
   },
   "source": [
    "We observe from the above table that there are several types of Named Entities with their own descriptions.\n",
    "\n",
    "Now, let's implement the same using both the NLTK and spaCy libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iynaiSBElQbf"
   },
   "source": [
    "### **NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1662461193871,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "WBdYh5UvLDAw",
    "outputId": "ce638f1b-fbc3-4d3f-8d82-df342e7871b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  When/WRB\n",
      "  (PERSON Sebastian/JJ Thrun/NNP)\n",
      "  started/VBD\n",
      "  working/VBG\n",
      "  on/IN\n",
      "  self-driving/JJ\n",
      "  cars/NNS\n",
      "  at/IN\n",
      "  (ORGANIZATION Google/NNP)\n",
      "  in/IN\n",
      "  2007/CD\n",
      "  ,/,\n",
      "  few/JJ\n",
      "  people/NNS\n",
      "  outside/IN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  company/NN\n",
      "  took/VBD\n",
      "  him/PRP\n",
      "  seriously/RB\n",
      "  ./.\n",
      "  I/PRP\n",
      "  can/MD\n",
      "  tell/VB\n",
      "  you/PRP\n",
      "  very/RB\n",
      "  senior/JJ\n",
      "  (ORGANIZATION CEOs/NNP)\n",
      "  of/IN\n",
      "  major/JJ\n",
      "  (GPE American/JJ)\n",
      "  car/NN\n",
      "  companies/NNS\n",
      "  would/MD\n",
      "  shake/VB\n",
      "  my/PRP$\n",
      "  hand/NN\n",
      "  and/CC\n",
      "  turn/VB\n",
      "  away/RB\n",
      "  because/IN\n",
      "  I/PRP\n",
      "  wasn/VBP\n",
      "  ’/JJ\n",
      "  t/NN\n",
      "  worth/NN\n",
      "  talking/VBG\n",
      "  to/TO\n",
      "  ,/,\n",
      "  said/VBD\n",
      "  (PERSON Thrun/NNP)\n",
      "  ,/,\n",
      "  in/IN\n",
      "  an/DT\n",
      "  interview/NN\n",
      "  with/IN\n",
      "  (PERSON Recode/NNP)\n",
      "  earlier/RBR\n",
      "  this/DT\n",
      "  week/NN)\n"
     ]
    }
   ],
   "source": [
    "# Using ne_chunk() method available in NLTK, we can recognize named entities using a classifier, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE.\n",
    "# First it will tokenize the whole corpus, and tag into its respective pos tags. In final step, each entity get identified into their respective entity tags.\n",
    "\n",
    "ne_tree = ne_chunk(nltk.pos_tag(word_tokenize(text)))\n",
    "\n",
    "# Displaying the results\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2aUqjqXvvtm"
   },
   "source": [
    "- We observe that each word present in the data is tagged to its respective named entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FznuD9udlTgk"
   },
   "source": [
    "### **spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1662461193871,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "1jWC-eBjlTvo",
    "outputId": "b055e87d-9568-455c-d5dc-4a861e242f13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named entities in the given text are\n",
      "\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun GPE\n",
      "Recode ORG\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "print(\"Named entities in the given text are\\n\")\n",
    "\n",
    "# doc is corpus, doc.ents gives available entites in that corpus\n",
    "\n",
    "for ent in doc.ents: \n",
    "\n",
    "    # Printing the entity text and its label\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEQRwImNn-Ek"
   },
   "source": [
    "- We can infer that **Sebastian Thrun** is a name of the **PERSON**, **2007** is a **DATE**, **American** is a **Nationality**, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ze6SPJ67KVN"
   },
   "source": [
    "## **Performance Comparison**\n",
    "\n",
    "Finally, let's take a quantitative final look at the performance / speed of both libraries. \n",
    "\n",
    "For instance, let us look at the Word Tokenization operation for simiplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIaCMTwd7KVN"
   },
   "source": [
    "### **Word Tokenization on NLTK vs spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3541,
     "status": "ok",
     "timestamp": 1662461197396,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "aSfeK2yP7KVN",
    "outputId": "caad4d8d-2fb7-4f8b-e7a1-f521029fc452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346 µs ± 24.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "Nw = %timeit -o nltk.tokenize.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3065,
     "status": "ok",
     "timestamp": 1662461200457,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "3lo5c7Yf7KVN",
    "outputId": "e00639b7-4f6a-43e5-ecb8-62ba3de5b3fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.5 µs ± 2.33 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# spaCy\n",
    "Sw = %timeit -o nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpLfEAq37bEF"
   },
   "source": [
    "It's clear to see that **the spaCy library takes much less computational time than NLTK to perform the same task on the same corpus.**\n",
    "\n",
    "This is quite true across the board, and spaCy is hence preferred in performance & deployment contexts for creating finalized NLP business solutions. NLTK, on the other hand, is preferred at the research & prototyping stage when coming up with Proof-of-Concepts, due to the lack of necessity of performance / speed at that stage and also its comprehensive features for all major NLP operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Mask510amIlTPRf4lPhfYzhhpXV3yZEL",
     "timestamp": 1662555562651
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
