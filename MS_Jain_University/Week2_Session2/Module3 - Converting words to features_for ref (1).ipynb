{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting words to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = pd.read_csv('imdb1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding custom stop words\n",
    "new_words=[]\n",
    "new_words = [\"some\",\"one\",\"like\",\"time\",\"br\",\"movie\",\"film\",\"could\",\"good\",'even', 'get', 'would',\n",
    "             'make', 'really', 'see', 'well', 'much', 'great', 'first', 'people', 'also', 'bad', \n",
    "             'show', 'way', 'thing', 'made', 'go', 'think', 'know', 'watch','look','many']\n",
    "stop_words = stop_words.union(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform text pre-processing in a loop for the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of reviews\n",
    "movie.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, movie.shape[0]):\n",
    "    #Remove punctuations\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', movie['review'][i])\n",
    "    \n",
    "    #Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    ##Convert to list from string\n",
    "    text = text.split()\n",
    "    \n",
    "    ##Lemmatizing\n",
    "    lm = WordNetLemmatizer() \n",
    "    text = [lm.lemmatize(word) for word in text if not word in stop_words] \n",
    "    text = \" \".join(text)\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hardly compare space adventure star war star trek fan star trek recognize pale comparison series trekkies ooze fact place back future better light term space travel story boy captured space raider pirate obviously fake unentertaining battle captured boy befriends pirate help slowly raider die end boy get return home last remaining pirate escape gravely wounded acting obvious total lack interesting dialogue effect storyline got 80 minute beyond want take shot involves space ahead warned'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the document term matrix - toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "* This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "\n",
    "* If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets understand count vectoriser\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use fit transform to transform a text corpus to a count vectoriser\n",
    "test_cv = cv.fit_transform([\"A wonderful production\", \n",
    "                            \"This was a wonderful way\", \n",
    "                            \"wonderful portrait about human relations\"])\n",
    "\n",
    "test_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View cv as an array\n",
    "count_vect_array = test_cv.toarray()\n",
    "count_vect_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'human',\n",
       " 'portrait',\n",
       " 'production',\n",
       " 'relations',\n",
       " 'this',\n",
       " 'was',\n",
       " 'way',\n",
       " 'wonderful']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the feature names of cv\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>human</th>\n",
       "      <th>portrait</th>\n",
       "      <th>production</th>\n",
       "      <th>relations</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "      <th>way</th>\n",
       "      <th>wonderful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   about  human  portrait  production  relations  this  was  way  wonderful\n",
       "0      0      0         0           1          0     0    0    0          1\n",
       "1      0      0         0           0          0     1    1    1          1\n",
       "2      1      1         1           0          1     0    0    0          1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Convert to dataframe\n",
    "count_vect_df = pd.DataFrame(count_vect_array, \n",
    "                             columns=cv.get_feature_names())\n",
    "count_vect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "about         1\n",
       "human         1\n",
       "portrait      1\n",
       "production    1\n",
       "relations     1\n",
       "this          1\n",
       "was           1\n",
       "way           1\n",
       "wonderful     3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How to get the word frequency for each term in the vocabulary\n",
    "count_vect_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie['review'] = movie['review'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "from nltk import word_tokenize \n",
    "tokens = word_tokenize(movie['review'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "bigrams = ngrams(tokens,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'first')\n",
      "('first', 'in')\n",
      "('in', 'the')\n",
      "('the', 'series')\n",
      "('series', 'was')\n",
      "('was', 'brilliant')\n",
      "('brilliant', ',')\n",
      "(',', 'easily')\n",
      "('easily', 'one')\n",
      "('one', 'of')\n",
      "('of', 'the')\n",
      "('the', 'best')\n",
      "('best', 'Horror')\n",
      "('Horror', 'films')\n",
      "('films', 'of')\n",
      "('of', 'all')\n",
      "('all', 'time')\n",
      "('time', '.')\n",
      "('.', 'This')\n",
      "('This', 'is')\n",
      "('is', 'the')\n",
      "('the', 'crappiest')\n",
      "('crappiest', '.')\n",
      "('.', 'When')\n",
      "('When', 'I')\n",
      "('I', 'sat')\n",
      "('sat', 'down')\n",
      "('down', 'to')\n",
      "('to', 'watch')\n",
      "('watch', 'this')\n",
      "('this', ',')\n",
      "(',', 'I')\n",
      "('I', 'was')\n",
      "('was', 'actually')\n",
      "('actually', 'thinking')\n",
      "('thinking', 'that')\n",
      "('that', 'how')\n",
      "('how', 'bad')\n",
      "('bad', 'the')\n",
      "('the', 'fourth')\n",
      "('fourth', 'and')\n",
      "('and', 'fifth')\n",
      "('fifth', 'ones')\n",
      "('ones', 'were')\n",
      "('were', ',')\n",
      "(',', 'this')\n",
      "('this', 'would')\n",
      "('would', 'have')\n",
      "('have', 'to')\n",
      "('to', 'be')\n",
      "('be', 'good')\n",
      "('good', 'after')\n",
      "('after', 'the')\n",
      "('the', 'previous')\n",
      "('previous', 'terrible')\n",
      "('terrible', 'ones')\n",
      "('ones', '.')\n",
      "('.', 'Boy')\n",
      "('Boy', 'was')\n",
      "('was', 'I')\n",
      "('I', 'wrong')\n",
      "('wrong', '.')\n",
      "('.', 'Incredibly')\n",
      "('Incredibly', 'wrong.')\n",
      "('wrong.', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', 'When')\n",
      "('When', 'I')\n",
      "('I', 'watched')\n",
      "('watched', 'the')\n",
      "('the', 'first')\n",
      "('first', 'ten')\n",
      "('ten', 'minutes')\n",
      "('minutes', 'of')\n",
      "('of', 'it')\n",
      "('it', ',')\n",
      "(',', 'I')\n",
      "('I', 'was')\n",
      "('was', 'actually')\n",
      "('actually', 'really')\n",
      "('really', 'tempted')\n",
      "('tempted', 'to')\n",
      "('to', 'turn')\n",
      "('turn', 'it')\n",
      "('it', 'off')\n",
      "('off', ',')\n",
      "(',', 'but')\n",
      "('but', 'I')\n",
      "('I', 'thought')\n",
      "('thought', 'no')\n",
      "('no', ',')\n",
      "(',', 'maybe')\n",
      "('maybe', 'it')\n",
      "('it', \"'ll\")\n",
      "(\"'ll\", 'improve')\n",
      "('improve', '.')\n",
      "('.', 'It')\n",
      "('It', \"didn't.\")\n",
      "(\"didn't.\", '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', 'Not')\n",
      "('Not', 'only')\n",
      "('only', 'is')\n",
      "('is', 'this')\n",
      "('this', 'just')\n",
      "('just', 'a')\n",
      "('a', 'dire')\n",
      "('dire', 'film')\n",
      "('film', 'by')\n",
      "('by', 'itself')\n",
      "('itself', ',')\n",
      "(',', 'it')\n",
      "('it', 'did')\n",
      "('did', \"n't\")\n",
      "(\"n't\", 'need')\n",
      "('need', 'another')\n",
      "('another', 'sequel')\n",
      "('sequel', ',')\n",
      "(',', 'because')\n",
      "('because', 'the')\n",
      "('the', 'last')\n",
      "('last', 'two')\n",
      "('two', '(')\n",
      "('(', 'fourth')\n",
      "('fourth', 'and')\n",
      "('and', 'fifth')\n",
      "('fifth', ')')\n",
      "(')', 'had')\n",
      "('had', 'already')\n",
      "('already', 'been')\n",
      "('been', 'terrible')\n",
      "('terrible', 'enough')\n",
      "('enough', '!')\n",
      "('!', 'Also')\n",
      "('Also', ',')\n",
      "(',', 'how')\n",
      "('how', 'many')\n",
      "('many', 'times')\n",
      "('times', 'can')\n",
      "('can', 'you')\n",
      "('you', 'bring')\n",
      "('bring', 'Freddy')\n",
      "('Freddy', 'back')\n",
      "('back', '!')\n",
      "('!', '?')\n",
      "('?', 'The')\n",
      "('The', 'acting')\n",
      "('acting', 'in')\n",
      "('in', 'it')\n",
      "('it', 'was')\n",
      "('was', 'TERRIBLE')\n",
      "('TERRIBLE', ',')\n",
      "(',', 'the')\n",
      "('the', 'story-line')\n",
      "('story-line', 'was')\n",
      "('was', 'predictable')\n",
      "('predictable', 'and')\n",
      "('and', 'crap')\n",
      "('crap', 'and')\n",
      "('and', 'it')\n",
      "('it', 'also')\n",
      "('also', 'had')\n",
      "('had', 'flaws')\n",
      "('flaws', 'in')\n",
      "('in', 'it')\n",
      "('it', 'as')\n",
      "('as', 'well')\n",
      "('well', '.')\n",
      "('.', 'The')\n",
      "('The', 'way')\n",
      "('way', 'they')\n",
      "('they', 'made')\n",
      "('made', 'Springwood')\n",
      "('Springwood', 'was')\n",
      "('was', 'just')\n",
      "('just', 'totally')\n",
      "('totally', 'wrong')\n",
      "('wrong', '.')\n",
      "('.', 'Pays')\n",
      "('Pays', 'no')\n",
      "('no', 'respect')\n",
      "('respect', 'to')\n",
      "('to', 'the')\n",
      "('the', 'first')\n",
      "('first', 'one')\n",
      "('one', 'at-all')\n",
      "('at-all', '.')\n",
      "('.', 'To')\n",
      "('To', 'add')\n",
      "('add', 'to')\n",
      "('to', 'this')\n",
      "('this', ',')\n",
      "(',', 'the')\n",
      "('the', 'whole')\n",
      "('whole', 'thing')\n",
      "('thing', 'seemed')\n",
      "('seemed', 'really')\n",
      "('really', 'over-the-top.')\n",
      "('over-the-top.', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', 'Some')\n",
      "('Some', 'people')\n",
      "('people', 'are')\n",
      "('are', 'saying')\n",
      "('saying', 'that')\n",
      "('that', 'this')\n",
      "('this', 'film')\n",
      "('film', 'was')\n",
      "('was', '``')\n",
      "('``', 'funny')\n",
      "('funny', \"''\")\n",
      "(\"''\", '.')\n",
      "('.', 'This')\n",
      "('This', 'film')\n",
      "('film', 'is')\n",
      "('is', 'not')\n",
      "('not', '``')\n",
      "('``', 'funny')\n",
      "('funny', \"''\")\n",
      "(\"''\", 'at')\n",
      "('at', 'all')\n",
      "('all', '.')\n",
      "('.', 'Since')\n",
      "('Since', 'when')\n",
      "('when', 'is')\n",
      "('is', 'Freddy')\n",
      "('Freddy', 'Krueger')\n",
      "('Krueger', 'supposed')\n",
      "('supposed', 'to')\n",
      "('to', 'be')\n",
      "('be', '``')\n",
      "('``', 'funny')\n",
      "('funny', \"''\")\n",
      "(\"''\", '?')\n",
      "('?', 'I')\n",
      "('I', 'would')\n",
      "('would', 'call')\n",
      "('call', 'it')\n",
      "('it', 'funnily')\n",
      "('funnily', 'crap')\n",
      "('crap', '.')\n",
      "('.', 'This')\n",
      "('This', 'film')\n",
      "('film', 'is')\n",
      "('is', 'supposed')\n",
      "('supposed', 'to')\n",
      "('to', 'be')\n",
      "('be', 'a')\n",
      "('a', 'Horror')\n",
      "('Horror', 'film')\n",
      "('film', ',')\n",
      "(',', 'not')\n",
      "('not', 'a')\n",
      "('a', 'comedy')\n",
      "('comedy', '.')\n",
      "('.', 'If')\n",
      "('If', 'Freddy')\n",
      "('Freddy', 'had')\n",
      "('had', 'a')\n",
      "('a', 'daughter')\n",
      "('daughter', ',')\n",
      "(',', 'would')\n",
      "('would', \"n't\")\n",
      "(\"n't\", 'that')\n",
      "('that', 'information')\n",
      "('information', 'have')\n",
      "('have', 'surfaced')\n",
      "('surfaced', 'like')\n",
      "('like', 'in')\n",
      "('in', 'the')\n",
      "('the', 'first')\n",
      "('first', 'one')\n",
      "('one', '!')\n",
      "('!', '?')\n",
      "('?', 'The')\n",
      "('The', 'ending')\n",
      "('ending', 'was')\n",
      "('was', 'also')\n",
      "('also', 'just')\n",
      "('just', 'plain')\n",
      "('plain', 'stupid')\n",
      "('stupid', 'and')\n",
      "('and', 'cheesy')\n",
      "('cheesy', ',')\n",
      "(',', 'exactly')\n",
      "('exactly', 'like')\n",
      "('like', 'the')\n",
      "('the', 'rest')\n",
      "('rest', 'of')\n",
      "('of', 'it')\n",
      "('it', '.')\n",
      "('.', 'This')\n",
      "('This', 'one')\n",
      "('one', 'completely')\n",
      "('completely', 'destroys')\n",
      "('destroys', 'the')\n",
      "('the', 'essence')\n",
      "('essence', 'and')\n",
      "('and', 'uniqueness')\n",
      "('uniqueness', 'of')\n",
      "('of', 'the')\n",
      "('the', 'first')\n",
      "('first', 'one')\n",
      "('one', '.')\n",
      "('.', 'Just')\n",
      "('Just', 'shows')\n",
      "('shows', 'itself')\n",
      "('itself', 'up.')\n",
      "('up.', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', 'Such')\n",
      "('Such', 'a')\n",
      "('a', 'shame')\n",
      "('shame', 'that')\n",
      "('that', 'Wes')\n",
      "('Wes', 'Craven')\n",
      "('Craven', 'created')\n",
      "('created', 'something')\n",
      "('something', 'so')\n",
      "('so', 'good')\n",
      "('good', 'in')\n",
      "('in', 'the')\n",
      "('the', 'beginning')\n",
      "('beginning', ',')\n",
      "(',', 'yet')\n",
      "('yet', 'it')\n",
      "('it', 'has')\n",
      "('has', 'to')\n",
      "('to', 'be')\n",
      "('be', 'dragged')\n",
      "('dragged', 'down')\n",
      "('down', 'because')\n",
      "('because', 'of')\n",
      "('of', 'this')\n",
      "('this', 'trash')\n",
      "('trash', 'that')\n",
      "('that', 'belongs')\n",
      "('belongs', 'in')\n",
      "('in', 'the')\n",
      "('the', 'bin')\n",
      "('bin', '.')\n",
      "('.', 'They')\n",
      "('They', 'should')\n",
      "('should', \"n't\")\n",
      "(\"n't\", 'have')\n",
      "('have', 'even')\n",
      "('even', 'bothered')\n",
      "('bothered', 'making')\n",
      "('making', 'this')\n",
      "('this', 'film')\n",
      "('film', '.')\n",
      "('.', 'Nor')\n",
      "('Nor', 'any')\n",
      "('any', 'of')\n",
      "('of', 'the')\n",
      "('the', 'other')\n",
      "('other', 'sequels')\n",
      "('sequels', ',')\n",
      "(',', 'except')\n",
      "('except', 'the')\n",
      "('the', 'third')\n",
      "('third', 'one')\n",
      "('one', '.')\n",
      "('.', 'The')\n",
      "('The', 'third')\n",
      "('third', 'one')\n",
      "('one', \"'s\")\n",
      "(\"'s\", 'the')\n",
      "('the', 'only')\n",
      "('only', 'decent')\n",
      "('decent', 'one')\n",
      "('one', 'out')\n",
      "('out', 'of')\n",
      "('of', 'all')\n",
      "('all', 'the')\n",
      "('the', 'sequels.')\n",
      "('sequels.', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', 'If')\n",
      "('If', 'this')\n",
      "('this', 'was')\n",
      "('was', 'a')\n",
      "('a', 'DVD')\n",
      "('DVD', 'by')\n",
      "('by', 'itself')\n",
      "('itself', 'and')\n",
      "('and', 'not')\n",
      "('not', 'part')\n",
      "('part', 'of')\n",
      "('of', 'the')\n",
      "('the', 'Nightmare')\n",
      "('Nightmare', 'On')\n",
      "('On', 'Elm')\n",
      "('Elm', 'Street')\n",
      "('Street', 'DVD')\n",
      "('DVD', 'set')\n",
      "('set', 'that')\n",
      "('that', 'I')\n",
      "('I', 'got')\n",
      "('got', ',')\n",
      "(',', 'I')\n",
      "('I', 'would')\n",
      "('would', 'have')\n",
      "('have', 'chucked')\n",
      "('chucked', 'it')\n",
      "('it', 'out')\n",
      "('out', 'when')\n",
      "('when', 'I')\n",
      "('I', 'got')\n",
      "('got', 'it.')\n",
      "('it.', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', 'Summary')\n",
      "('Summary', ':')\n",
      "(':', 'A')\n",
      "('A', 'pathetic')\n",
      "('pathetic', 'and')\n",
      "('and', 'poor')\n",
      "('poor', 'attempt')\n",
      "('attempt', 'at')\n",
      "('at', 'a')\n",
      "('a', 'sequel.')\n",
      "('sequel.', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', '-')\n",
      "('-', 'a')\n",
      "('a', 'complete')\n",
      "('complete', 'MOCKERY')\n",
      "('MOCKERY', 'of')\n",
      "('of', 'the')\n",
      "('the', 'first')\n",
      "('first', 'film')\n",
      "('film', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', '<')\n",
      "('<', 'br')\n",
      "('br', '/')\n",
      "('/', '>')\n",
      "('>', 'So')\n",
      "('So', 'please')\n",
      "('please', ',')\n",
      "(',', 'do')\n",
      "('do', \"n't\")\n",
      "(\"n't\", 'waste')\n",
      "('waste', 'your')\n",
      "('your', 'time')\n",
      "('time', 'on')\n",
      "('on', 'this')\n",
      "('this', 'worthless')\n",
      "('worthless', 'junk')\n",
      "('junk', '.')\n"
     ]
    }
   ],
   "source": [
    "for b in bigrams:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = ngrams(tokens,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'first', 'in')\n",
      "('first', 'in', 'the')\n",
      "('in', 'the', 'series')\n",
      "('the', 'series', 'was')\n",
      "('series', 'was', 'brilliant')\n",
      "('was', 'brilliant', ',')\n",
      "('brilliant', ',', 'easily')\n",
      "(',', 'easily', 'one')\n",
      "('easily', 'one', 'of')\n",
      "('one', 'of', 'the')\n",
      "('of', 'the', 'best')\n",
      "('the', 'best', 'Horror')\n",
      "('best', 'Horror', 'films')\n",
      "('Horror', 'films', 'of')\n",
      "('films', 'of', 'all')\n",
      "('of', 'all', 'time')\n",
      "('all', 'time', '.')\n",
      "('time', '.', 'This')\n",
      "('.', 'This', 'is')\n",
      "('This', 'is', 'the')\n",
      "('is', 'the', 'crappiest')\n",
      "('the', 'crappiest', '.')\n",
      "('crappiest', '.', 'When')\n",
      "('.', 'When', 'I')\n",
      "('When', 'I', 'sat')\n",
      "('I', 'sat', 'down')\n",
      "('sat', 'down', 'to')\n",
      "('down', 'to', 'watch')\n",
      "('to', 'watch', 'this')\n",
      "('watch', 'this', ',')\n",
      "('this', ',', 'I')\n",
      "(',', 'I', 'was')\n",
      "('I', 'was', 'actually')\n",
      "('was', 'actually', 'thinking')\n",
      "('actually', 'thinking', 'that')\n",
      "('thinking', 'that', 'how')\n",
      "('that', 'how', 'bad')\n",
      "('how', 'bad', 'the')\n",
      "('bad', 'the', 'fourth')\n",
      "('the', 'fourth', 'and')\n",
      "('fourth', 'and', 'fifth')\n",
      "('and', 'fifth', 'ones')\n",
      "('fifth', 'ones', 'were')\n",
      "('ones', 'were', ',')\n",
      "('were', ',', 'this')\n",
      "(',', 'this', 'would')\n",
      "('this', 'would', 'have')\n",
      "('would', 'have', 'to')\n",
      "('have', 'to', 'be')\n",
      "('to', 'be', 'good')\n",
      "('be', 'good', 'after')\n",
      "('good', 'after', 'the')\n",
      "('after', 'the', 'previous')\n",
      "('the', 'previous', 'terrible')\n",
      "('previous', 'terrible', 'ones')\n",
      "('terrible', 'ones', '.')\n",
      "('ones', '.', 'Boy')\n",
      "('.', 'Boy', 'was')\n",
      "('Boy', 'was', 'I')\n",
      "('was', 'I', 'wrong')\n",
      "('I', 'wrong', '.')\n",
      "('wrong', '.', 'Incredibly')\n",
      "('.', 'Incredibly', 'wrong.')\n",
      "('Incredibly', 'wrong.', '<')\n",
      "('wrong.', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', '<')\n",
      "('>', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', 'When')\n",
      "('>', 'When', 'I')\n",
      "('When', 'I', 'watched')\n",
      "('I', 'watched', 'the')\n",
      "('watched', 'the', 'first')\n",
      "('the', 'first', 'ten')\n",
      "('first', 'ten', 'minutes')\n",
      "('ten', 'minutes', 'of')\n",
      "('minutes', 'of', 'it')\n",
      "('of', 'it', ',')\n",
      "('it', ',', 'I')\n",
      "(',', 'I', 'was')\n",
      "('I', 'was', 'actually')\n",
      "('was', 'actually', 'really')\n",
      "('actually', 'really', 'tempted')\n",
      "('really', 'tempted', 'to')\n",
      "('tempted', 'to', 'turn')\n",
      "('to', 'turn', 'it')\n",
      "('turn', 'it', 'off')\n",
      "('it', 'off', ',')\n",
      "('off', ',', 'but')\n",
      "(',', 'but', 'I')\n",
      "('but', 'I', 'thought')\n",
      "('I', 'thought', 'no')\n",
      "('thought', 'no', ',')\n",
      "('no', ',', 'maybe')\n",
      "(',', 'maybe', 'it')\n",
      "('maybe', 'it', \"'ll\")\n",
      "('it', \"'ll\", 'improve')\n",
      "(\"'ll\", 'improve', '.')\n",
      "('improve', '.', 'It')\n",
      "('.', 'It', \"didn't.\")\n",
      "('It', \"didn't.\", '<')\n",
      "(\"didn't.\", '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', '<')\n",
      "('>', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', 'Not')\n",
      "('>', 'Not', 'only')\n",
      "('Not', 'only', 'is')\n",
      "('only', 'is', 'this')\n",
      "('is', 'this', 'just')\n",
      "('this', 'just', 'a')\n",
      "('just', 'a', 'dire')\n",
      "('a', 'dire', 'film')\n",
      "('dire', 'film', 'by')\n",
      "('film', 'by', 'itself')\n",
      "('by', 'itself', ',')\n",
      "('itself', ',', 'it')\n",
      "(',', 'it', 'did')\n",
      "('it', 'did', \"n't\")\n",
      "('did', \"n't\", 'need')\n",
      "(\"n't\", 'need', 'another')\n",
      "('need', 'another', 'sequel')\n",
      "('another', 'sequel', ',')\n",
      "('sequel', ',', 'because')\n",
      "(',', 'because', 'the')\n",
      "('because', 'the', 'last')\n",
      "('the', 'last', 'two')\n",
      "('last', 'two', '(')\n",
      "('two', '(', 'fourth')\n",
      "('(', 'fourth', 'and')\n",
      "('fourth', 'and', 'fifth')\n",
      "('and', 'fifth', ')')\n",
      "('fifth', ')', 'had')\n",
      "(')', 'had', 'already')\n",
      "('had', 'already', 'been')\n",
      "('already', 'been', 'terrible')\n",
      "('been', 'terrible', 'enough')\n",
      "('terrible', 'enough', '!')\n",
      "('enough', '!', 'Also')\n",
      "('!', 'Also', ',')\n",
      "('Also', ',', 'how')\n",
      "(',', 'how', 'many')\n",
      "('how', 'many', 'times')\n",
      "('many', 'times', 'can')\n",
      "('times', 'can', 'you')\n",
      "('can', 'you', 'bring')\n",
      "('you', 'bring', 'Freddy')\n",
      "('bring', 'Freddy', 'back')\n",
      "('Freddy', 'back', '!')\n",
      "('back', '!', '?')\n",
      "('!', '?', 'The')\n",
      "('?', 'The', 'acting')\n",
      "('The', 'acting', 'in')\n",
      "('acting', 'in', 'it')\n",
      "('in', 'it', 'was')\n",
      "('it', 'was', 'TERRIBLE')\n",
      "('was', 'TERRIBLE', ',')\n",
      "('TERRIBLE', ',', 'the')\n",
      "(',', 'the', 'story-line')\n",
      "('the', 'story-line', 'was')\n",
      "('story-line', 'was', 'predictable')\n",
      "('was', 'predictable', 'and')\n",
      "('predictable', 'and', 'crap')\n",
      "('and', 'crap', 'and')\n",
      "('crap', 'and', 'it')\n",
      "('and', 'it', 'also')\n",
      "('it', 'also', 'had')\n",
      "('also', 'had', 'flaws')\n",
      "('had', 'flaws', 'in')\n",
      "('flaws', 'in', 'it')\n",
      "('in', 'it', 'as')\n",
      "('it', 'as', 'well')\n",
      "('as', 'well', '.')\n",
      "('well', '.', 'The')\n",
      "('.', 'The', 'way')\n",
      "('The', 'way', 'they')\n",
      "('way', 'they', 'made')\n",
      "('they', 'made', 'Springwood')\n",
      "('made', 'Springwood', 'was')\n",
      "('Springwood', 'was', 'just')\n",
      "('was', 'just', 'totally')\n",
      "('just', 'totally', 'wrong')\n",
      "('totally', 'wrong', '.')\n",
      "('wrong', '.', 'Pays')\n",
      "('.', 'Pays', 'no')\n",
      "('Pays', 'no', 'respect')\n",
      "('no', 'respect', 'to')\n",
      "('respect', 'to', 'the')\n",
      "('to', 'the', 'first')\n",
      "('the', 'first', 'one')\n",
      "('first', 'one', 'at-all')\n",
      "('one', 'at-all', '.')\n",
      "('at-all', '.', 'To')\n",
      "('.', 'To', 'add')\n",
      "('To', 'add', 'to')\n",
      "('add', 'to', 'this')\n",
      "('to', 'this', ',')\n",
      "('this', ',', 'the')\n",
      "(',', 'the', 'whole')\n",
      "('the', 'whole', 'thing')\n",
      "('whole', 'thing', 'seemed')\n",
      "('thing', 'seemed', 'really')\n",
      "('seemed', 'really', 'over-the-top.')\n",
      "('really', 'over-the-top.', '<')\n",
      "('over-the-top.', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', '<')\n",
      "('>', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', 'Some')\n",
      "('>', 'Some', 'people')\n",
      "('Some', 'people', 'are')\n",
      "('people', 'are', 'saying')\n",
      "('are', 'saying', 'that')\n",
      "('saying', 'that', 'this')\n",
      "('that', 'this', 'film')\n",
      "('this', 'film', 'was')\n",
      "('film', 'was', '``')\n",
      "('was', '``', 'funny')\n",
      "('``', 'funny', \"''\")\n",
      "('funny', \"''\", '.')\n",
      "(\"''\", '.', 'This')\n",
      "('.', 'This', 'film')\n",
      "('This', 'film', 'is')\n",
      "('film', 'is', 'not')\n",
      "('is', 'not', '``')\n",
      "('not', '``', 'funny')\n",
      "('``', 'funny', \"''\")\n",
      "('funny', \"''\", 'at')\n",
      "(\"''\", 'at', 'all')\n",
      "('at', 'all', '.')\n",
      "('all', '.', 'Since')\n",
      "('.', 'Since', 'when')\n",
      "('Since', 'when', 'is')\n",
      "('when', 'is', 'Freddy')\n",
      "('is', 'Freddy', 'Krueger')\n",
      "('Freddy', 'Krueger', 'supposed')\n",
      "('Krueger', 'supposed', 'to')\n",
      "('supposed', 'to', 'be')\n",
      "('to', 'be', '``')\n",
      "('be', '``', 'funny')\n",
      "('``', 'funny', \"''\")\n",
      "('funny', \"''\", '?')\n",
      "(\"''\", '?', 'I')\n",
      "('?', 'I', 'would')\n",
      "('I', 'would', 'call')\n",
      "('would', 'call', 'it')\n",
      "('call', 'it', 'funnily')\n",
      "('it', 'funnily', 'crap')\n",
      "('funnily', 'crap', '.')\n",
      "('crap', '.', 'This')\n",
      "('.', 'This', 'film')\n",
      "('This', 'film', 'is')\n",
      "('film', 'is', 'supposed')\n",
      "('is', 'supposed', 'to')\n",
      "('supposed', 'to', 'be')\n",
      "('to', 'be', 'a')\n",
      "('be', 'a', 'Horror')\n",
      "('a', 'Horror', 'film')\n",
      "('Horror', 'film', ',')\n",
      "('film', ',', 'not')\n",
      "(',', 'not', 'a')\n",
      "('not', 'a', 'comedy')\n",
      "('a', 'comedy', '.')\n",
      "('comedy', '.', 'If')\n",
      "('.', 'If', 'Freddy')\n",
      "('If', 'Freddy', 'had')\n",
      "('Freddy', 'had', 'a')\n",
      "('had', 'a', 'daughter')\n",
      "('a', 'daughter', ',')\n",
      "('daughter', ',', 'would')\n",
      "(',', 'would', \"n't\")\n",
      "('would', \"n't\", 'that')\n",
      "(\"n't\", 'that', 'information')\n",
      "('that', 'information', 'have')\n",
      "('information', 'have', 'surfaced')\n",
      "('have', 'surfaced', 'like')\n",
      "('surfaced', 'like', 'in')\n",
      "('like', 'in', 'the')\n",
      "('in', 'the', 'first')\n",
      "('the', 'first', 'one')\n",
      "('first', 'one', '!')\n",
      "('one', '!', '?')\n",
      "('!', '?', 'The')\n",
      "('?', 'The', 'ending')\n",
      "('The', 'ending', 'was')\n",
      "('ending', 'was', 'also')\n",
      "('was', 'also', 'just')\n",
      "('also', 'just', 'plain')\n",
      "('just', 'plain', 'stupid')\n",
      "('plain', 'stupid', 'and')\n",
      "('stupid', 'and', 'cheesy')\n",
      "('and', 'cheesy', ',')\n",
      "('cheesy', ',', 'exactly')\n",
      "(',', 'exactly', 'like')\n",
      "('exactly', 'like', 'the')\n",
      "('like', 'the', 'rest')\n",
      "('the', 'rest', 'of')\n",
      "('rest', 'of', 'it')\n",
      "('of', 'it', '.')\n",
      "('it', '.', 'This')\n",
      "('.', 'This', 'one')\n",
      "('This', 'one', 'completely')\n",
      "('one', 'completely', 'destroys')\n",
      "('completely', 'destroys', 'the')\n",
      "('destroys', 'the', 'essence')\n",
      "('the', 'essence', 'and')\n",
      "('essence', 'and', 'uniqueness')\n",
      "('and', 'uniqueness', 'of')\n",
      "('uniqueness', 'of', 'the')\n",
      "('of', 'the', 'first')\n",
      "('the', 'first', 'one')\n",
      "('first', 'one', '.')\n",
      "('one', '.', 'Just')\n",
      "('.', 'Just', 'shows')\n",
      "('Just', 'shows', 'itself')\n",
      "('shows', 'itself', 'up.')\n",
      "('itself', 'up.', '<')\n",
      "('up.', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', '<')\n",
      "('>', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', 'Such')\n",
      "('>', 'Such', 'a')\n",
      "('Such', 'a', 'shame')\n",
      "('a', 'shame', 'that')\n",
      "('shame', 'that', 'Wes')\n",
      "('that', 'Wes', 'Craven')\n",
      "('Wes', 'Craven', 'created')\n",
      "('Craven', 'created', 'something')\n",
      "('created', 'something', 'so')\n",
      "('something', 'so', 'good')\n",
      "('so', 'good', 'in')\n",
      "('good', 'in', 'the')\n",
      "('in', 'the', 'beginning')\n",
      "('the', 'beginning', ',')\n",
      "('beginning', ',', 'yet')\n",
      "(',', 'yet', 'it')\n",
      "('yet', 'it', 'has')\n",
      "('it', 'has', 'to')\n",
      "('has', 'to', 'be')\n",
      "('to', 'be', 'dragged')\n",
      "('be', 'dragged', 'down')\n",
      "('dragged', 'down', 'because')\n",
      "('down', 'because', 'of')\n",
      "('because', 'of', 'this')\n",
      "('of', 'this', 'trash')\n",
      "('this', 'trash', 'that')\n",
      "('trash', 'that', 'belongs')\n",
      "('that', 'belongs', 'in')\n",
      "('belongs', 'in', 'the')\n",
      "('in', 'the', 'bin')\n",
      "('the', 'bin', '.')\n",
      "('bin', '.', 'They')\n",
      "('.', 'They', 'should')\n",
      "('They', 'should', \"n't\")\n",
      "('should', \"n't\", 'have')\n",
      "(\"n't\", 'have', 'even')\n",
      "('have', 'even', 'bothered')\n",
      "('even', 'bothered', 'making')\n",
      "('bothered', 'making', 'this')\n",
      "('making', 'this', 'film')\n",
      "('this', 'film', '.')\n",
      "('film', '.', 'Nor')\n",
      "('.', 'Nor', 'any')\n",
      "('Nor', 'any', 'of')\n",
      "('any', 'of', 'the')\n",
      "('of', 'the', 'other')\n",
      "('the', 'other', 'sequels')\n",
      "('other', 'sequels', ',')\n",
      "('sequels', ',', 'except')\n",
      "(',', 'except', 'the')\n",
      "('except', 'the', 'third')\n",
      "('the', 'third', 'one')\n",
      "('third', 'one', '.')\n",
      "('one', '.', 'The')\n",
      "('.', 'The', 'third')\n",
      "('The', 'third', 'one')\n",
      "('third', 'one', \"'s\")\n",
      "('one', \"'s\", 'the')\n",
      "(\"'s\", 'the', 'only')\n",
      "('the', 'only', 'decent')\n",
      "('only', 'decent', 'one')\n",
      "('decent', 'one', 'out')\n",
      "('one', 'out', 'of')\n",
      "('out', 'of', 'all')\n",
      "('of', 'all', 'the')\n",
      "('all', 'the', 'sequels.')\n",
      "('the', 'sequels.', '<')\n",
      "('sequels.', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', '<')\n",
      "('>', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', 'If')\n",
      "('>', 'If', 'this')\n",
      "('If', 'this', 'was')\n",
      "('this', 'was', 'a')\n",
      "('was', 'a', 'DVD')\n",
      "('a', 'DVD', 'by')\n",
      "('DVD', 'by', 'itself')\n",
      "('by', 'itself', 'and')\n",
      "('itself', 'and', 'not')\n",
      "('and', 'not', 'part')\n",
      "('not', 'part', 'of')\n",
      "('part', 'of', 'the')\n",
      "('of', 'the', 'Nightmare')\n",
      "('the', 'Nightmare', 'On')\n",
      "('Nightmare', 'On', 'Elm')\n",
      "('On', 'Elm', 'Street')\n",
      "('Elm', 'Street', 'DVD')\n",
      "('Street', 'DVD', 'set')\n",
      "('DVD', 'set', 'that')\n",
      "('set', 'that', 'I')\n",
      "('that', 'I', 'got')\n",
      "('I', 'got', ',')\n",
      "('got', ',', 'I')\n",
      "(',', 'I', 'would')\n",
      "('I', 'would', 'have')\n",
      "('would', 'have', 'chucked')\n",
      "('have', 'chucked', 'it')\n",
      "('chucked', 'it', 'out')\n",
      "('it', 'out', 'when')\n",
      "('out', 'when', 'I')\n",
      "('when', 'I', 'got')\n",
      "('I', 'got', 'it.')\n",
      "('got', 'it.', '<')\n",
      "('it.', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', '<')\n",
      "('>', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', 'Summary')\n",
      "('>', 'Summary', ':')\n",
      "('Summary', ':', 'A')\n",
      "(':', 'A', 'pathetic')\n",
      "('A', 'pathetic', 'and')\n",
      "('pathetic', 'and', 'poor')\n",
      "('and', 'poor', 'attempt')\n",
      "('poor', 'attempt', 'at')\n",
      "('attempt', 'at', 'a')\n",
      "('at', 'a', 'sequel.')\n",
      "('a', 'sequel.', '<')\n",
      "('sequel.', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', '<')\n",
      "('>', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', '-')\n",
      "('>', '-', 'a')\n",
      "('-', 'a', 'complete')\n",
      "('a', 'complete', 'MOCKERY')\n",
      "('complete', 'MOCKERY', 'of')\n",
      "('MOCKERY', 'of', 'the')\n",
      "('of', 'the', 'first')\n",
      "('the', 'first', 'film')\n",
      "('first', 'film', '<')\n",
      "('film', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', '<')\n",
      "('>', '<', 'br')\n",
      "('<', 'br', '/')\n",
      "('br', '/', '>')\n",
      "('/', '>', 'So')\n",
      "('>', 'So', 'please')\n",
      "('So', 'please', ',')\n",
      "('please', ',', 'do')\n",
      "(',', 'do', \"n't\")\n",
      "('do', \"n't\", 'waste')\n",
      "(\"n't\", 'waste', 'your')\n",
      "('waste', 'your', 'time')\n",
      "('your', 'time', 'on')\n",
      "('time', 'on', 'this')\n",
      "('on', 'this', 'worthless')\n",
      "('this', 'worthless', 'junk')\n",
      "('worthless', 'junk', '.')\n"
     ]
    }
   ],
   "source": [
    "for b in trigrams:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a vocabulary of words for the movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count vectoriser arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore words that appear in 80% of documents, \n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(stop_words=stop_words)\n",
    "X=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 22296)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(stop_words=stop_words,\n",
    "                  ngram_range=(1,3))\n",
    "X=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 392989)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to change the number of words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directly provide a dimension using max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(max_features=10000,\n",
    "                   stop_words=stop_words,\n",
    "                  ngram_range=(1,3))\n",
    "X=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore terms that are present in most documents (custom stopwords) - Using max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(max_df = 0.7,\n",
    "                   stop_words=stop_words,\n",
    "                  ngram_range=(1,3))\n",
    "X=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 392989)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore terms that are rare in the corpus - Using min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(min_df = 0.01,\n",
    "                   stop_words=stop_words,\n",
    "                  ngram_range=(1,3))\n",
    "X=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1674)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us view a part of the DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>10 10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>yet another</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>young man</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1674 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  10 10  100  11  12  13  15  20  30  40  ...  yes  yet  yet another  \\\n",
       "0   0      0    0   0   0   0   0   0   0   0  ...    0    0            0   \n",
       "1   0      0    0   0   0   0   0   0   0   0  ...    0    1            0   \n",
       "2   0      0    0   0   0   0   0   0   0   0  ...    0    0            0   \n",
       "3   0      0    0   0   0   1   0   0   0   0  ...    0    0            0   \n",
       "4   0      0    0   0   0   0   0   0   0   0  ...    0    0            0   \n",
       "\n",
       "   york  young  young man  younger  youth  zero  zombie  \n",
       "0     0      0          0        0      0     0       0  \n",
       "1     0      0          0        0      0     0       0  \n",
       "2     0      0          0        0      0     0       0  \n",
       "3     0      0          0        0      0     0       0  \n",
       "4     0      0          0        0      0     0       0  \n",
       "\n",
       "[5 rows x 1674 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_array = X.toarray()\n",
    "\n",
    "\n",
    "### Convert to dataframe\n",
    "\n",
    "count_vect_df = pd.DataFrame(count_vect_array, columns=cv.get_feature_names())\n",
    "count_vect_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Lets understand count vectoriser\n",
    "tf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use fir transform to transform a text corpus to a count vectoriser\n",
    "test_tf = tf.fit_transform([\"A wonderful production\", \n",
    "                            \"This was a wonderful way\", \n",
    "                            \"wonderful portrait about human relations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.861037  , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.50854232],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.54645401, 0.54645401, 0.54645401, 0.32274454],\n",
       "       [0.47952794, 0.47952794, 0.47952794, 0.        , 0.47952794,\n",
       "        0.        , 0.        , 0.        , 0.28321692]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View cv as an array\n",
    "tf_array = test_tf.toarray()\n",
    "tf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>human</th>\n",
       "      <th>portrait</th>\n",
       "      <th>production</th>\n",
       "      <th>relations</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "      <th>way</th>\n",
       "      <th>wonderful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.861037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.508542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.322745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.479528</td>\n",
       "      <td>0.479528</td>\n",
       "      <td>0.479528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      about     human  portrait  production  relations      this       was  \\\n",
       "0  0.000000  0.000000  0.000000    0.861037   0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000    0.000000   0.000000  0.546454  0.546454   \n",
       "2  0.479528  0.479528  0.479528    0.000000   0.479528  0.000000  0.000000   \n",
       "\n",
       "        way  wonderful  \n",
       "0  0.000000   0.508542  \n",
       "1  0.546454   0.322745  \n",
       "2  0.000000   0.283217  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the feature names of cv\n",
    "tf.get_feature_names()\n",
    "\n",
    "tf_df = pd.DataFrame(tf_array,columns=tf.get_feature_names() )\n",
    "tf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us compare this with the dataframe from count vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>human</th>\n",
       "      <th>portrait</th>\n",
       "      <th>production</th>\n",
       "      <th>relations</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "      <th>way</th>\n",
       "      <th>wonderful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   about  human  portrait  production  relations  this  was  way  wonderful\n",
       "0      0      0         0           1          0     0    0    0          1\n",
       "1      0      0         0           0          0     1    1    1          1\n",
       "2      1      1         1           0          1     0    0    0          1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying tf-idf vectoriser to the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Fit the tf-idf model\n",
    "tfv = TfidfVectorizer(stop_words=stop_words, \n",
    "                      ngram_range=(1,3), \n",
    "                      min_df = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and build vocab\n",
    "tfmat = tfv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=tfv.get_feature_names()\n",
    "#feature_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23815"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000 000</th>\n",
       "      <th>01</th>\n",
       "      <th>06</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>10 000</th>\n",
       "      <th>10 10</th>\n",
       "      <th>10 10 star</th>\n",
       "      <th>...</th>\n",
       "      <th>zip</th>\n",
       "      <th>zippy</th>\n",
       "      <th>zoey</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombie appear</th>\n",
       "      <th>zombie attack</th>\n",
       "      <th>zombie chronicle</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone episode</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  23815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  000 000   01   06   08   10  10 000  10 10  10 10 star  ...  zip  \\\n",
       "0  0.0  0.0      0.0  0.0  0.0  0.0  0.0     0.0    0.0         0.0  ...  0.0   \n",
       "1  0.0  0.0      0.0  0.0  0.0  0.0  0.0     0.0    0.0         0.0  ...  0.0   \n",
       "2  0.0  0.0      0.0  0.0  0.0  0.0  0.0     0.0    0.0         0.0  ...  0.0   \n",
       "3  0.0  0.0      0.0  0.0  0.0  0.0  0.0     0.0    0.0         0.0  ...  0.0   \n",
       "4  0.0  0.0      0.0  0.0  0.0  0.0  0.0     0.0    0.0         0.0  ...  0.0   \n",
       "\n",
       "   zippy  zoey  zombie  zombie appear  zombie attack  zombie chronicle  zone  \\\n",
       "0    0.0   0.0     0.0            0.0            0.0               0.0   0.0   \n",
       "1    0.0   0.0     0.0            0.0            0.0               0.0   0.0   \n",
       "2    0.0   0.0     0.0            0.0            0.0               0.0   0.0   \n",
       "3    0.0   0.0     0.0            0.0            0.0               0.0   0.0   \n",
       "4    0.0   0.0     0.0            0.0            0.0               0.0   0.0   \n",
       "\n",
       "   zone episode  zoom  \n",
       "0           0.0   0.0  \n",
       "1           0.0   0.0  \n",
       "2           0.0   0.0  \n",
       "3           0.0   0.0  \n",
       "4           0.0   0.0  \n",
       "\n",
       "[5 rows x 23815 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect_array = tfmat.toarray()\n",
    "\n",
    "\n",
    "### Convert to dataframe\n",
    "\n",
    "tfidf_vect_df = pd.DataFrame(tfidf_vect_array, columns=tfv.get_feature_names())\n",
    "tfidf_vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
